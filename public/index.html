<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    .subgrid {
  grid-column: screen;
  display: grid;
  grid-template-columns: inherit;
  grid-template-rows: inherit;
  grid-column-gap: inherit;
  grid-row-gap: inherit;
}

d-figure.base-grid {
  grid-column: screen;
  background: hsl(0, 0%, 97%);
  padding: 20px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
  margin-bottom: 1em;
  position: relative;
}

d-figure > figure {
  margin-top: 0;
  margin-bottom: 20;
}

d-figure.attackAnim .param-space-acc .acc-dif-hist {
  grid-column: text;
  display: block;
  margin-left: auto;
  margin-right: auto;
}

d-figure.acc-dif-hist .tick line {
  visibility: visible;
}

/* d-figure.param-space-acc {
  grid-column: text;
  display: block;
  margin-left: auto;
  margin-right: auto;
} */

d-figure.poison-demo {
  position: relative;
  grid-column: text;
  display: block;
  width: 984px;
  height: 684;
  margin-left: auto;
  margin-right: auto;
}

.hidden {
  visibility: hidden;
}

.poison-demo-controls {
  position: absolute;
  width: 454px;
  height: 150px;
  left: 20px;
  /* left: calc(100% - 964px - 20px); */
  /* top: 435px; */
  top: 485px;
}

.poison-demo-controls-text {
  position: absolute;
  width: calc(100% - 200px + 10px);
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  text-anchor: middle;
  pointer-events: none;
}

.shaded-figure {
  background-color: hsl(0, 0%, 97%);
  border-top: 1px solid hsla(0, 0%, 0%, 0.1);
  border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
  padding: 30px 0;
}

.pointer {
  position: absolute;
  width: 26px;
  height: 26px;
  top: 26px;
  left: -48px;
}

.blue-point {
  fill: steelblue;
  opacity: 0.6;
  /* r: 4; */
}

.red-point {
  fill: orangered;
  opacity: 0.7;
  /* r: 4; */
}

.target-point {
  fill: orange;
  /* fill: blueviolet; */
  /* fill: steelblue; */
  opacity: 1;
  /* transform: scale(1.25, 1.25); */
  /* r: 5; */
}

.selected-point {
  fill: orange;
  /* fill: steelblue; */
  opacity: 0.5;
  /* transform: scale(1.25, 1.25); */
  /* r: 5; */
}

.blue-poison {
  fill: steelblue;
  /* opacity: 0.5; */
}

.red-poison {
  fill: orangered;
  /* opacity: 0.5; */
}

.blue-delete {
  fill: black;
}

.red-delete {
  fill: black;
}

.area-blue {
  fill: lightsteelblue;
  opacity: 0.4;
  stroke-width: 0;
}

.area-red {
  fill: red;
  opacity: 0.2;
  stroke-width: 0;
}

.overlay {
  position: absolute;
}

.slider {
  position: absolute;
  border-radius: 2px;
  -webkit-appearance: none;
  grid-column: middle;
}

.attack-slider {
  height: 16px;
  background: #d3d3d3;
  outline: none;
  opacity: 0.7;
  -webkit-animation: 0.2s;
  transform: opacity 0.2s;
  left: 80px;
  width: calc(100% - 80px - 80px);
  top: calc(100%);
}
.attack-slider::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 16px;
  height: 32px;
  border-radius: 25%;
  background: #3d3d3d;
  cursor: pointer;
}
.attack-slider::-moz-range-thumb {
  width: 16px;
  height: 32px;
  border-radius: 25%;
  background: #3d3d3d;
  cursor: pointer;
}

.slider-container {
  position: absolute;
  width: 100%;
}

.mode-container {
  position: absolute;
  width: 180px;
  left: calc(100% - 180px);
  /* left: calc(100% - 230px); */
}

.control-slider {
  width: calc(100% - 200px);
  height: 16px;
  padding: 0;
  margin: 0;
  top: 24px;
  opacity: 0.7;
}
.control-slider::-webkit-slider-runnable-track {
  height: 6.4px;
  cursor: pointer;
  box-shadow: 1px 1px 1px #808080, 0px 0px 1px #b0b0b0;
  background: #d3d3d3;
  border-radius: 18px;
  border: 0.1px solid #a0a0a0;
}
.control-slider::-moz-range-track {
  height: 6.4px;
  cursor: pointer;
  box-shadow: 1px 1px 1px #808080, 0px 0px 1px #b0b0b0;
  background: #d3d3d3;
  border-radius: 18px;
  border: 0.1px solid #a0a0a0;
}
.control-slider::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 16px;
  height: 16px;
  border-radius: 40%;
  background: #3d3d3d;
  cursor: pointer;
  margin-top: calc(0.5 * (6.4px - 16px));
}
.control-slider::-moz-range-thumb {
  width: 16px;
  height: 16px;
  border-radius: 40%;
  background-color: #3d3d3d;
  cursor: pointer;
}

/* .slider-container {
  background-color: #f2f2f9;
} */

/* .histogram-slider { */
  /* position: absolute; */
  /* border: 1px solid #AAB; */
  /* background: #BCE; */
  /* height: 100%; */
  /* width: 58px; */
  /* top: 0px; */
  /* cursor: move; */
/* } */

.histogram-slider {
  height: 16px;
  background: #d3d3d3;
  outline: none;
  opacity: 0.7;
  -webkit-animation: 0.2s;
  transform: opacity 0.2s;
  left: 40px;
  width: calc(100% - 40px - 40px);
  cursor: move;
}

.histogram-slider .slider {
  background-color: #9AC;
  height:100%;
}

/* .histogram-slider .box {
  background-color: #9AC;
} */

.histogram-slider .handle {
  position: absolute;
  height: 32px;
  width: 9px;
  /* border: 1px solid #AAB; */
  background: #9AC;

  /* Support for bootstrap */
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}

.histogram-slider .EE {
  right: -4px;
  cursor: e-resize;
}

.histogram-slider .WW {
  cursor: w-resize;
  left: -4px;
}

.histogram-slider .EE, .histogram-slider .WW {
  top: 50%;
  margin-top: -16px;
}

.button {
  position: absolute;
  height: 24px;
  opacity: 0.7;
  background-color: #d3d3d3;
  border-radius: 8px;
  display: block;
  align-items: center;
  -webkit-appearance: none;
}

.play-button {
  top: calc(100% - 2px);
  left: 50px;
}

.step-forward-button {
  top: calc(100% - 2px);
  right: calc(50px - 2.5px);
}

.step-back-button {
  top: calc(100% - 2px);
  left: calc(50px - 32px);
}

.control-button {
  position: relative;
  margin-right: 7.5px;
  width: auto;
  min-width: 40px;
  height: 48px;
  cursor: pointer;
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
}

.poison-counter {
  position: absolute;
  height: 24px;
  top: calc(100% - 40px);
  margin: auto;
}

.tick line {
  visibility: hidden;
}

.colorbar .tick line, .histogram-slider .tick line {
  visibility: visible;
}

.input-container {
  position: absolute;
}

.input {
  position: absolute;
  border: 0;
  border-bottom: 2px solid #aaa;
  background: transparent;
  -webkit-appearance: none;
}
.input::-webkit-outer-spin-button,
.input::-webkit-inner-spin-button {
  -webkit-appearance: none;
  margin: 0;
}
.input[type="number"] {
  -moz-appearance: textfield;
}
.input:focus {
  border: none;
  outline: none;
  border-bottom: 2px solid #222;
}

.seed-label {
  position: absolute;
  left: calc(100% - 200px - 115px);
  top: 14px;
  font-family: Arial, Helvetica, sans-serif;
  font-size: 10px;
  font-weight: bold;
  color: #808080;
  pointer-events: none;
  transition: all 0.5s ease-in-out;
}

.mode-label {
  position: absolute;
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
}

.seed-textfield {
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  left: calc(100% - 200px - 115px);
  width: calc(-250px + 200px + 115px);
  /* width: 100px; */
}

.seed-button {
  left: calc(100% - 200px - 100px - 150px - 10px);
  /* width: 150px; */
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  cursor: pointer;
}

.seed-container {
  position: absolute;
  width: 100%;
}

.demo-buttons-container {
  display: flex;
  justify-content: center;
  position: absolute;
  /* left: calc(100% - 230px); */
  left: calc(100% - 180px);
  top: 32px;
  width: calc(100% - 180px);
  height: calc(100% - 32px);
}

.summary-box-container, .demo-stats-container {
  position: absolute;
  font-family: Arial, Helvetica, sans-serif;
  line-height: 1.2em;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.summary-box-container {
  pointer-events: visible;
}

.summary-box-scrollarea {
  overflow-y: auto;
  overflow-x: hidden;
  margin: 10px;
  width: calc(100% - 20px);
  height: calc(100% - 20px);
}

.summary-box-entry {
  margin-right: 5px;
  display: block;
  padding:0px;
  width:calc(100% - 5px);
}

.attack-summary-container {
  overflow-y: auto;
  margin-top: 5px;
  margin-left: 5px;
  margin-right: 10px;
}
  
.attack-summary-container .item {
  display: block;
  float: left;
  margin-right: 5px;
  margin-bottom: 10px;
  width: calc(33% - 5px);
  height: auto;
}

.demo-stats-title {
  text-align: left;
  margin-left: 6px;
  font-weight: bold;
  font-size: medium;
  margin-bottom: 2px;
}

.demo-stats-entry {
  text-align: left;
  margin-left: 6px;
  margin-right: 6px;
  font-size: small;
  margin-bottom: 0px;
}

.demo-stats-entry .right {
  text-align: right;
}

/* .demo-stats-container {
  position: absolute;
  text-align: left;
  pointer-events: none;
  margin: 1px;
  width: 170px;
  height: auto;
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  line-height: 12px;
  background-color: rgba(255, 255, 255, 0.9);

  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
} */

.unselectable {
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.demo-stats-text {
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  line-height: 12px;
}

.fig-title {
  font-family: Arial, Helvetica, sans-serif;
  font-size: large;
}

.fig-label-text {
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  /* font-weight: bold; */
  /* line-height: 12px; */
}
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      {
    "title": "Poisoning Attacks and Subpopulation Susceptibility",
    "description": "Experiments on subpopulations vulnerable to poisoning attacks",
    "authors": [
        {
            "author": "Evan Rose",
            "affiliation": "University of Virginia",
            "authorURL": "",
            "affiliationURL": "https://engineering.virginia.edu/departments/computer-science"
        }
    ],
    "katex": {
        "delimiters": [
            {
                "left": "$",
                "right": "$",
                "display": false
            },
            {
                "left": "$$",
                "right": "$$",
                "display": true
            }
        ]
    }
}
    </script>
  </d-front-matter>

  <d-title>
    <h1>Poisoning Attacks and Subpopulation Susceptibility</h1>
    <p>Experiments on subpopulations vulnerable to poisoning attacks.</p>
  </d-title>

  <d-figure id="svelte-poison-demo-dfigure" class="poison-demo">
    <figure>
      <div id="svelte-demo-target" style="text-align: center"></div>
      <div id="svelte-demo-controls" class="poison-demo-controls">
        <div id="alpha-container" style="top:0px;" class="slider-container">
          <label id="alphaText" for="alphaSlider" class="poison-demo-controls-text">Class Separation α = 1.00</label>
          <input id="alphaSlider" type="range" class="slider control-slider" min="0" max="12" value="4" />
        </div>
        <div id="beta-container" style="top:54px;" class="slider-container">
          <label id="betaText" for="betaSlider" class="poison-demo-controls-text">Random Label Fraction β = 0.00</label>
          <input id="betaSlider" type="range" class="slider control-slider" min="0" max="10" value="0" />
        </div>
        <div id="seed-container" style="top:116px;" class="seed-container">
          <input id="seedButton" type="button" class="button seed-button" value="Randomize Dataset" />
          <label for="seedField" class="seed-label">Dataset seed</label>
          <input id="seedField" type="number" class="input seed-textfield" value="1" />
        </div>
        <div style="position:absolute; right:188px; height: 100%; width:1.5px; background-color:#808080;"></div>
        <div id="mode-container" class="mode-container">
          <label for="mode" class="mode-label">Poisoning algorithm:</label>
          <select style="position:absolute; left:140px; top:4px" id="attackAlgo" name="Attack Algorithm">
            <option value="manual">Manual</option>
            <option value="labelFlip">Label flip</option>
            <option value="modelTargeted">Model-targeted</option>
          </select>
        </div>
        <div id="manual-buttons" class="demo-buttons-container">
          <button id="labelButton" class="button control-button">
            <div>
              Label:
            </div>
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M0 5Q0 10 5 10 10 10 10 5 10 0 5 0 0 0 0 5" fill="steelblue" />
            </svg>
          </button>
          <button id="toolButton" class="button control-button">
            <div>
              Tool:
            </div>
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.2145 7.1365-.0113 9.9037 2.7559 8.678l-1.5415-1.5415zM1.5838 6.6299 7.0865 1.1336l1.6781 1.6799L3.2617 8.3098zM9.2027 2.3757l.3494-.3493A1.1871 1.1871 90 107.8736.3477L7.5238.6961z" fill="#1f1f1f" />
            </svg>
          </button>
          <button id="resetButton" class="button control-button">
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.5 2 1.5 10 8.5 10 8.5 2 7.5 2 7.5 9 2.5 9 2.5 2ZM0.5 1 9.5 1 9.5 2 0.5 2ZM4 1 6 1 6 0 6 0 4 0ZM3 3 3 8 4 8 4 3ZM6 3 6 8 7 8 7 3ZM4.5 3 4.5 8 5.5 8 5.5 3Z" fill="#1f1f1f" />
            </svg>
          </button>
        </div>
        <div id="algorithm-buttons" class="demo-buttons-container" style="visibility: hidden;">
          <button id="algorithmPlayButton" class="button control-button" style="cursor: pointer">
            <svg width="10" height="10" viewBox="0 0 10 10">
              <path d="M0 0L0 10L3 10L3 0ZM6 0L6 10L9 10L9 0Z" ; fill="#1f1f1f" />
            </svg>
          </button>
          <button id="algorithmResetButton" class="button control-button">
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.5 2 1.5 10 8.5 10 8.5 2 7.5 2 7.5 9 2.5 9 2.5 2ZM0.5 1 9.5 1 9.5 2 0.5 2ZM4 1 6 1 6 0 6 0 4 0ZM3 3 3 8 4 8 4 3ZM6 3 6 8 7 8 7 3ZM4.5 3 4.5 8 5.5 8 5.5 3Z" fill="#1f1f1f" />
            </svg>
          </button>
        </div>
      </div>
      <figcaption style="position:absolute; width: 380px; top:485px; left:594px; text-align: left;">
        Machine learning is susceptible to poisoning attacks in which adversaries inject poisoned training data into the training set, usually to induce specific model behavior.

        In one type of attack, the subpopulation attack, the attacker's goal is to compromise the model's behavior only on a particular subset of the input space.

        When attackers focus their efforts towards subpopulations of the data distribution, which subpopulations are the most vulnerable to an attack?
        <figcaption>
    </figure>
  </d-figure>

  <!--
  <button bind:this={playButton} class="button play-button" style="cursor: pointer">
    <svg width="10" height="10" viewBox="0 0 10 10">
      <path d={svgPaths.pausePath} fill="#888" />
    </svg>
  </button>
  <button bind:this={stepForwardButton} class="button step-forward-button" style="cursor: pointer">
    <svg width="10" height="10" viewBox="0 0 10 10">
      <path d={svgPaths.stepForwardPath} fill="#888" />
    </svg>
  </button>
  <button bind:this={stepBackButton} class="button step-back-button" style="cursor: pointer">
    <svg width="10" height="10" viewBox="0 0 10 10">
      <path d={svgPaths.stepBackPath} fill="#888" />
    </svg>
  </button> -->

  <d-article>
    <p>
      Machine learning is susceptible to poisoning attacks, in which an attacker controls a small fraction of the training data and chooses that data to induce some undesired behavior in the trained model<d-cite key="10.5555/1387709.1387716, biggio2012poisoning"></d-cite>. Previous works have mostly considered two attacker objectives: indiscriminate attacks, where the attacker's goal is to reduce overall model accuracy<d-cite key="biggio2012poisoning, 10.5555/3007337.3007488, 10.5555/2886521.2886721, steinhardt2017certified, koh2018stronger"></d-cite>, and instance-targeted attacks, where the attacker's goal is to reduce accuracy on a specific instance<d-cite key="shafahi2018poison, zhu2019transferable, koh2017understanding, geiping2020witches, huang2020metapoison"></d-cite>. Recently, <d-cite key="jagielski2021subpopulation"></d-cite> introduced the subpopulation attack, a more realistic setting in which the adversary attempts to control the model’s behavior on a specific subpopulation while not affecting the model’s behavior on instances outside the subpopulation. In this article, we explore a question about poisoning attacks against subpopulations of a data distribution: how do subpopulation characteristics affect attack difficulty? We will attempt to visually understand these attacks by animating a poisoning attack algorithm in a simplified setting, and additionally try to describe the difficulty of the attacks in terms of the properties of the subpopulations they are against.
    </p>

    <h1>Problem Setup</h1>

    <p>
      We use the same problem setup as <d-cite key="suya2021modeltargeted"></d-cite> in their online model-targeted poisoning attack. In particular, we consider a binary prediction task $h : \mathcal{X} \rightarrow \mathcal{Y},$ where $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} = \{+1, -1\},$ and where the prediction model $h$ is characterized by the model parameters $\theta \in \Theta \subseteq \mathbb{R}^d.$ We denote the non-negative convex loss on a point $(x, y) \in \mathcal{X} \times \mathcal{Y}$ by $l(\theta; x, y)$, and define the loss over a set of points $A$ as $L(\theta; A) = \sum_{(x, y) \in A} l(\theta; x, y)$. Finally, we also describe the poisoning attack process with the following game-theoretic formalization:
    </p>

    <ol>
      <li>
        $N$ data points are drawn from the true data distribution over $\mathcal{X} \times \mathcal{Y}$ to produce the clean training set $\mathcal{D}_c.$
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c$ and the model space $\Theta$, generates a target classifier $\theta_p \in \Theta$ which satisfies the attack objective.
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c,$ $\Theta,$ $\theta_p,$ and the training process, produces a set of poisoning points $\mathcal{D}_p.$
      </li>
      <li>
        The model builder trains on $\mathcal{D}_c \cup \mathcal{D}_p$ to produce the induced classifier $\theta_{atk}.$
      </li>
    </ol>

    <p>
      We next describe the components we use in our experiments for each part of the above framework, as well as the formation of the subpopulations.
    </p>

    <h3>Datasets</h3>

    <p>
      We study poisoning attacks against two different types of datasets. First, we generate synthetic datasets using dataset generation algorithms from Scikit-learn <d-cite key="scikit-learn"></d-cite>. Each of these datasets is controlled by a set of parameters, which we use to control global dataset properties. The synthetic datasets are also limited to just two features, so that a direct visualization of the attack is possible. Second, we use the UCI Adult dataset <d-cite key="Dua:2019"></d-cite>, which has been used previously in evaluations of poisoning attacks <d-cite key="suya2021modeltargeted, jagielski2021subpopulation"></d-cite>. The Adult dataset is of much higher dimension (57 after data transformations), and so the attack process cannot be visualized directly as with the synthetic datasets. The purpose of this dataset is to gauge the attack behavior in a more complicated setting.
    </p>

    <h3>Subpopulation Formation and Adversary Goal</h3>

    <p>
      We consider two subpopulation generation processes: clustering and semantic generation. Cluster subpopulations are chosen by using a k-means clustering algorithm to divide the training set into different clusters (ClusterMatch in <d-cite key="jagielski2021subpopulation"></d-cite>) and then taking only the instances with a negative label from each cluster. The attacker objective is to have the induced model misclassify the entire subpopulation as the positive label. This method is the same as used in <d-cite key="suya2021modeltargeted"></d-cite>, and is useful since it allows us to measure the success of the attack without any ambiguity as the accuracy of the induced model on the subpopulation; that is, a completely successful attack attains 0% test accuracy on the target subpopulation, and in general the attack success may be measured as the error rate on the target subpopulation.
    </p>

    <p>
      In the semantic generation setting, the adversary cares about some semantic property possessed by certain instances in the dataset. Semantic subpopulations are generated by using a feature-matching algorithm (FeatureMatch in <d-cite key="jagielski2021subpopulation"></d-cite>) to obtain those instances which satisfy the semantic property and again taking only the instances with a negative label. This subpopulation generation process reflects a more realistic attacker objective, since an attacker is likely to care about influencing model behavior on a subpopulation matching a set of meaningful properties. However, this process relies on the existence of semantically meaningful features in the dataset, and so cannot be performed on our synthetic datasets.
    </p>

    <h3>Training Algorithm</h3>

    <p>
      We assume the model builder trains models using empirical risk minimization (ERM) with the following optimization strategy:
    </p>

    <d-math style="margin: auto" block>
      \theta_c = \underset{\theta \in \Theta}{\text{argmin}} \frac{1}{|\mathcal{D}_c|} L(\theta; \mathcal{D}_c) + C_r \cdot R(\theta)
    </d-math>

    <p>
      where $R(\cdot)$ is the non-negative regularization function and $C_r$ is the regularization strength parameter.
    </p>

    <h3>Target Model</h3>

    <p>
      We conduct experiments on linear SVM models. The target model for each attack was generated using the label-flipping attack variant described in <d-cite key="koh2018stronger"></d-cite>, which was also used by <d-cite key="suya2021modeltargeted"></d-cite> in their experiments. As opposed to a more general attack setting, a label-flipping attack only allows the adversary to change some fraction of the labels in the clean dataset. In the variant described by <d-cite key="koh2018stronger"></d-cite>, the adversary chooses examples from the clean dataset, flips their labels, and repeats them any number of times to produce the poisoned dataset. The target model was chosen to be the induced model resulting from this label-flipping attack. More specifically, for each subpopulation, we use the label-flipping attack to generate a collection of candidate classifiers which each achieve 0% accuracy (100% error rate) on the target subpopulation. Afterwards, the classifier with the lowest loss on the clean dataset is chosen as the target model.
    </p>

    <h3>Attack Algorithm</h3>

    <p>
      We use the online model-targeted attack algorithm developed by <d-cite key="suya2021modeltargeted"></d-cite>. Given the clean dataset, target model, and model training parameters, the algorithm produces a set of poisoning points by maintaining an intermediate induced model and choosing points which maximize the loss difference between the intermediate model and target model. As each poisoning point is chosen, the intermediate model is updated using a new set of points (now including the newly chosen poison point) using the knowledge of the model training process. Importantly, the online attack provides theoretical guarantees about convergence of the induced model to the target model as the number of poisoning points increases. Since we have chosen our target model to misclassify the entire subpopulation, this means we have a guarantee that the online attack process will eventually produce a poisoned dataset which induces a satisfactory target model.
    </p>

    <p>
      A useful consequence of using the online attack algorithm is that the rate of convergence is characterized by the loss difference between the target and clean models on the clean dataset. If we define the <em>loss-based distance</em> $D_{l, \mathcal{X}, \mathcal{Y}} : \Theta \times \Theta \rightarrow \mathbb{R}$ between two models $\theta_1, \theta_2$ over a space $\mathcal{X} \times \mathcal{Y}$ with loss function $l(\theta; x, y)$ by
    </p>

    <d-math style="margin: auto" block>
      D_{l, \mathcal{X}, \mathcal{Y}}(\theta_1, \theta_2) := \max_{(x, y) \in \mathcal{X} \times \mathcal{Y}} l(\theta_1; x, y) - l(\theta_2; x, y),
    </d-math>

    <p>
      then the loss-based distance between the induced model $\theta_{atk}$ and the target model $\theta_p$ correlates to the loss difference $L(\theta_p; \mathcal{D}_c) - L(\theta_c; D_c)$ between $\theta_p$ and the clean model $\theta_c$ on the clean dataset. This fact gives a general heuristic for predicting attack difficulty: the closer a target model is to the clean model as measured by loss difference on the clean dataset, the easier the attack will be. This also justifies the decision to choose a target model with lower loss on the clean dataset.
    </p>

    <p>
      Why use the online attack algorithm instead of a simpler one, like the label-flipping attack? In our experiments, the model-targeted attacks were in general much more efficient than the label-flipping attacks. Our eventual goal is to describe attack difficulty as measured by the number of poisons used, so our attack should serve as a proxy for the most efficient attack against the subpopulation. In this regard, the model-targeted attack is a much better choice than the label-flipping attack. Moreover, the model-targeted attack has nice theoretical properties and is a general enough framework to be worthy of study on its own.
    </p>

    <p>
      Now that we have outlined the attack process, we can move on to our first set of experiments on synthetic datasets.
    </p>

    <h1>Synthetic Datasets</h1>

    <!-- <figure style="text-align: center" id="demo2">
      <svg height="448px" width="448px">
        <image xlink:href="images/datasets.png" height="448" width="448"></image>
      </svg>
      <figcaption>
        We generate several synthetic datasets controlled by two parameters: a <em>class separation</em> parameter, which controls the distance between class centers, and a <em>label flip</em> parameter, which controls the amount of label noise present in the dataset.
      </figcaption>
    </figure> -->

    <h3>Synthetic Dataset Generation</h3>

    <p>
      For the first set of experiments, we consider a family of synthetic datasets resembling Gaussian mixtures with two components. The datasets are controlled by two parameters, which we will refer to as the dataset parameters. The first dataset parameter is a class separation parameter $\alpha \ge 0$ which controls the distance between class centers. The second dataset parameter is a label noise parameter $\beta \in [0, 1]$ which controls the fraction of points whose labels are randomly assigned. Fixing the dataset parameters, we generate several datasets using different random seeds. Each of the datasets is generated with $n=3000$ points and is class-balanced, before being divided into train and test sets in a 2:1 ratio.
    </p>

    <h3>Dataset Parameter Space</h3>

    <p>
      We generate the datasets over a grid of the dataset parameters. For each combination of parameters, ten synthetic datasets are created using different random seeds. Before attacking the subpopulations, let’s observe the behavior of the clean models trained on each combination of the dataset parameters.
    </p>

    <d-figure id="svelte-param-space-acc-dfigure" class="param-space-acc">
      <figure>
        <div id="svelte-param-space-acc-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>The clean model performance is affected by the dataset parameters: as the classes become more separated and less noisy, the classifier is able to better separate them. Click points on the plot to see the datasets generated with each parameter combination.</figcaption>
      </figure>
    </d-figure>

    <p>
      The overall trends in the plot make intuitive sense: clean model accuracy improves as the classes become more separated and exhibit less label noise.
    </p>

    <!-- <p>
      On the left and upper edges of the plot, we see that the clean model performs very poorly on datasets with either high label noise or small class separation. One hypothesis we might develop is that attacks against subpopulations within these datasets are easy: since the loss difference between any two models will be close to zero, the attack should be effective with only a few poison points. On the other hand, datasets with accurate clean models should produce subpopulations that are harder to attack, since the loss difference between an accurate model and an inaccurate one is high.
    </p> -->

    <h3>Attack Visualization</h3>

    <p>
      We can now move onto visualizing the poisoning attacks on these datasets. For each synthetic dataset, we run the k-means clustering algorithm (k=16) and extract the negative-label instances from each cluster to form the subpopulations. We then generate target models for each nonempty subpopulation according to the earlier specification (100% error rate on the target subpopulation, measured on the test set). Finally, we use the online attack to generate a set of poisons which induce the desired behavior. The attack completes when the induced model misclassifies at least 50% of the target subpopulation, measured on the training set.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example1" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>The poisons are added sequentially to induce a model which misclassifies the orange points as the positive label.</figcaption>
    </d-figure>

    <p>
      The above animation shows how the online attack sequntially adds points into the dataset to move the induced model towards the target model. Take note of how the positive-labeled poisons work to reorient the model's decision boundary to cover the subpopulation while minimizing the impact on the rest of the dataset. This behavior reflects the part of the target model selection criteria which minimizes the loss on the clean dataset. Another possibility would have been to push the entire decision boundary downwards without reorienting it<d-footnote>i.e., to use a target model which only differs from the clean model in the bias term.</d-footnote>, but this would have resulted in a model with higher loss on other parts of the dataset, and thus a higher loss difference to the clean model. Intuitively, such an alternative would experience more "resistance" from the dataset, preventing the induced model from moving as swiftly to the target.
    </p>

    <!-- <p>
      With the ability to visualize poisoning attacks, we are now equipped to analyze some of the subpopulation properties that determine attack difficulty.
    </p> -->

    <d-figure id="svelte-scatterplot-dfigure-example2" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against a dataset with high label noise is easy, since the clean model is not heavily influenced by the points already in the training set.</figcaption>
    </d-figure>

    <p>
      Let's look at a low-difficulty attack. The above dataset exhibits a large amount of label noise, and as a result the clean model performs very poorly. When poisoning points are added, the model is quickly persuaded to change its decision on the target subpopulation. Observe that the poisoned model labels all points in the dataset as the positive label, with the decision boundary lying out of view. In other words, the attack is changing not just the model behavior local to the target subpopulation, but also the global behavior of the model.
    </p>

    <!-- <p>
      If we lower the label noise slightly, the attack does not induce quite as strong a response from the intermediate models,
    </p> -->

    <d-figure id="svelte-scatterplot-dfigure-example3" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against a dataset with close class centers is easy, since the linear SVM model does not have sufficent capacity to perform well on the clean dataset.</figcaption>
    </d-figure>

    <p>
      The next example shows a situation similar to the noisy dataset, but now for a slightly different reason: although the data now form meaningful classes, the model does not have enough capacity to create a useful classification function. The result is the same: poison points strongly influence the behavior of the model. Note that in both of the above examples, it was important that the classes be balanced; if the labels were represented in a different ratio, the clean model would have preferred to classify all points as the dominant label, and the attack may not have been as easy.
    </p>

    <p>
      As mentioned earlier, the properties that apply to the above two datasets should apply to all the datasets with either close class centers or high label noise. We can demonstrate this empirically by looking at the mean attack difficulty over the entire grid of dataset parameters:
    </p>

    <d-figure id="svelte-param-space-dif-dfigure" class="param-space-acc">
      <figure>
        <div id="svelte-param-space-dif-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Average attack difficulty is roughly characterized by clean model accuracy, with more accurate models requiring more difficult attacks. Click points on the plot to see different dataset parameter combinations, or click datasets on the right to see specific attacks.</figcaption>
      </figure>
    </d-figure>

    <p>
      It appears that the attacks against the datasets with poor clean models tend to be the easiest, and in general attack difficulty increases with clean model accuracy. The behavior is the most consistent with low accuracy clean models, since all attacks are easy and require few poisons. As the clean model accuracy increases, the behvaior becomes more complex and attack difficulty begins to depend on the properties of the subpopulation. Since attack difficulty is driven by the loss difference between the target and clean models, it makes sense to try to characterize the behavior of further examples in terms of this quantity.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example4" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack with low collateral damage is easy, since the loss difference between the target and clean models is small.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim4">
      <video width="448" height="448" controls>
        <source src="animations/anim4.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack with low collateral damage is easy, since the loss difference between the target and clean models is small.
      </figcaption>
    </figure> -->

    <p>
      The next attack is easy for the attacker despite being against a dataset with an accurate clean model. The targeted subpopulation lies on the boundary of the class it belongs to, and more specifically the loss difference between the target and clean models is small. In other words, the attack causes very little "collateral damage."
    </p>

    <!-- <p>
      This example possesses a few interesting properties, in addition to its low difficulty. First, the clean and induced models have decision boundaries with significantly different orientations. 
    </p> -->

    <d-figure id="svelte-scatterplot-dfigure-example5" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against the center of a cluster is hard, since the subpopulation is "protected" by surrounding data.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim5">
      <video width="448" height="448" controls>
        <source src="animations/anim5.mp4" type="video/mp4">
      </video>
      <figcaption>
        Caption
      </figcaption>
    </figure> -->

    <p>
      This is our first example of an exceptionally difficult attack, requiring over 800 poisons. The loss difference between the target and clean models is high, since the target model misclassifies a large number of points in the negative class. Notice that it seems hard to even produce a target model which does not cause a large amount of collateral damage. In some sense, the subpopulation is protected against attacks due it its central location.
    </p>

    <h3>Quantitative Analysis</h3>

    <p>
      Visualizations are helpful, but are rarely possible given the high-dimensional nature of most datasets. Now that we have visualized some of the attacks in the lower-dimensional setting, can we quantify the properties that describe the difficulty of poisoning attacks?
    </p>

    <p>
      Earlier we made the obseravation that attack difficulty tends to increase with clean model accuracy. To be a little more precise, we can see how the attack difficulty distribution changes as a function of clean model accuracy, for our attack setup.
    </p>

    <p>
      We describe the difficulty of an attack against a dataset with $n$ datapoints which uses $p$ poisoning points as simply the ratio $p/n$. That is, the difficulty of an attack is the fraction of training points the attacker adds to the dataset.
    </p>

    <d-figure id="svelte-acc-dif-hist-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-acc-dif-hist-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Datasets with accurate clean models tend to produce attacks with a wider range of difficulties. Datasets with inaccurate clean models tend to produce easy attacks.</figcaption>
      </figure>
    </d-figure>

    <h1>Adult Dataset</h1>

    <h1>Discussion</h1>

    <p>
      A natural objection to this analysis is that the linear SVM is not a sufficiently complex model to produce meaningful results about subpopulation susceptibility. We offer two rebuttals: the first is that simple, low-capacity models are still used widely in practice due to their ease of use, low computational cost, and effectiveness<d-cite key="decrema2019progress, tramer2021differentially"></d-cite>, and so our simplified analysis is still relevant in practice. The second is that models may use kernel methods to first project input data into a higher-dimensional space and then use a hyperplane to classify the transformed input. Assuming our observations about the importance of spatial relationships within datasets hold in higher dimensional spaces, the same conclusions reached in our simplified setting still apply to the more complex setting by examining the spatial relationships in the transformed space.
    </p>

    <p>
      Another objection is that the model-targeted attack may not adequately describe the difficulty of any attack against a subpopulation, since a more optimal attack may exist using a different target model or different attack framework.
    </p>

    <p>
      One open question is how subpopulation susceptibility depends on bias in the training data. In our experiments, all datasets were class-balanced. What happens when this is not the case? Do subpopulations from parts of the input space lacking representation in the training data admit easier poisoning attacks?
    </p>

    <h1>Conclusion</h1>

  </d-article>



  <d-appendix>
    <!-- <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to...
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>