<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    .subgrid {
  grid-column: screen; 
  display: grid; 
  grid-template-columns: inherit;
  grid-template-rows: inherit;
  grid-column-gap: inherit;
  grid-row-gap: inherit;
}

d-figure.base-grid {
  grid-column: screen;
  background: hsl(0, 0%, 97%);
  padding: 20px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
  margin-bottom: 1em;
  position: relative;
}

d-figure > figure {
  margin-top: 0;
  margin-bottom: 0;
}

.shaded-figure {
  background-color: hsl(0, 0%, 97%);
  border-top: 1px solid hsla(0, 0%, 0%, 0.1);
  border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
  padding: 30px 0;
}

.pointer {
  position: absolute;
  width: 26px;
  height: 26px;
  top: 26px;
  left: -48px;
}

  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      {
    "title": "Poisoning Attacks and Subpopulation Susceptibility",
    "description": "Experiments on subpopulations vulnerable to poisoning attacks",
    "authors": [
        {
            "author": "Evan Rose",
            "affiliation": "University of Virginia",
            "authorURL": "",
            "affiliationURL": "https://engineering.virginia.edu/departments/computer-science"
        }
    ],
    "katex": {
        "delimiters": [
            {
                "left": "$",
                "right": "$",
                "display": false
            },
            {
                "left": "$$",
                "right": "$$",
                "display": true
            }
        ]
    }
}
    </script>
  </d-front-matter>

  <d-title>
    <h1>Poisoning Attacks and Subpopulation Susceptibility</h1>
    <p>Experiments on subpopulations vulnerable to poisoning attacks.</p>
  </d-title>

  <figure style="position: relative; margin: auto; width: 984px; height: 425px">
    <div id="demo" style="position:relative; height:300px; border: 1px solid rgba(0, 0, 0, 0.2); background-color: rgba(0, 0, 0, 0.2); text-align: center">Interactive attack demo: the reader can generate datasets using the same parameters used in the experiments, create subpopulations, and try to manually add poisons to achieve the objective</div>
    <div id="controls" style="position: absolute; width: 300px; height: 50px; left: 20px; top: 320px">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Demo Controls here?</text>
    </div>
    <!-- <div id="sliderAlpha" style="position: absolute; width: 300px; height: 50px; left: 20px; top: 320px">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Class Separation $d = 1.0$</text>
    </div>
    <div id="sliderBeta" style="position:absolute; width: 300px; height: 50px; left: 280px; top: 320px;;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Label Noise $p = 0.2$</text>
    </div> -->
    <figcaption id="demo" style="position:absolute; width: 500px; top: 320px; left: 484px">
      Machine learning is susceptible to poisoning attacks where adversaries inject poisoned training data into the training set, usually to induce specific model behavior. When attackers focus their efforts towards subpopulations of the data distribution, which subpopulations are the most vulnerable to an attack?
    </figcaption>
  </figure>

  <d-article>
    <p>Note: the figures in this article are temporary and should be replaced with interactive ones.</p>

    <!-- <h1>Introduction</h1> -->

    <p>
      With the growing success of machine learning in a wide variety of applications, there is a temptation to involve machine learning systems in critical areas. Probably the most popular example of such an application is in self-driving cars, where computer vision powered by machine learning turns into automated decisions made by the vehicle. In these scenarios, the cost of failure is high, so every part of the system must be reliable.
    </p>

    <p>
      The usual machine learning process generally relies on a particular set of assumptions. Understanding these assumptions, as well as the ways they may fail to hold, is crucial to developing robust machine learning. The most important of these assumptions describes the relationship between model performance and training data: that a model that performs well on the training data should perform similarly in deployment.
    </p>
    
    <p>
      This assumption, while useful in practice, is faulty for a few reasons. First, even when the training data extensively covers the space of valid inputs, the resulting model can still be fragile and sensitive to slight perturbations of inputs, especially in the presence of a principled adversary<d-cite key="szegedy2013intriguing, su2017pixel"></d-cite>. Second, it is difficult to ensure that the training data adequately represents the data the model will see in deployment. It is possible for the data to either insufficiently cover the entire space of valid inputs, or for the training data to incorrectly represent the true distribution of inputs.
    </p>

    <p>
      The invalidity of this core assumption in these cases gives way to a variety of vulnerabilities inherent to machine learning systems. If machine learning is to be used in critical applications, we should demand higher standards for robustness from machine learning systems which account for these vulnerabilities.
    </p>

    <p>
      In this article, we focus on a class of attacks against machine learning systems known as poisoning attacks, and more specifically we examine these attacks as they apply against subpopulations of an input space. We will first attempt to visually understand these attacks by animating an poisoning attack algorithm in a simplified setting, and later try to describe the difficulty of the attacks in terms of the properties of the subpopulations they are against.
    </p>

    <h3>Adversarial Machine Learning</h3>

    <p>
      Adversarial machine learning seeks to understand machine learning under the presence of adversaries who use knowledge about the machine learning process to create an attack<d-cite key="10.1145/2046684.2046692"></d-cite>. Often, the objective of such an attack is to compromise the behavior of the model. Attacks can be classified based on when they occur: a train time attack occurs during the training of the model, while a test time (or inference time) attack occurs after the model has already been trained. A common example of an attack is against email spam classification. As a train time attack, the adversary (the spam author) may create benign emails which closely resemble spam emails in the hopes that a model trained on the benign emails will later misclassify the spam emails as benign. As a test time attack, the adversary may augment their spam emails to include phrases commonly found in benign emails, with the goal of confusing the already-trained model. The first of these attacks is an example of a poisoning attack, which is our focus in this article.
    </p>

    <h3>Poisoning Attacks</h3>

    <p>
      In a poisoning attack, an adversary controls some fraction of the training data and attempts to choose that data as to induce some undesired behavior in the trained model. Different threat models give differing levels of knowledge and control to the adversary, such as knowledge of the clean training data and the ability to modify existing points. We consider an addition-only attack with full knowledge of the training data and training algorithm.
    </p>

    <p>
      Poisoning attacks have existed for a long time <d-cite key="10.5555/1387709.1387716, biggio2012poisoning"></d-cite>, but the threat of poisoning attacks is increasing with the size of the datasets used to train models. Machine learning systems rely on the ability to collect large amounts of training data, often coming from several untrusted sources. When models are trained with this data, it is often infeasible to completely sanitize and verify the data due to its large quantity. If the data source has some public-facing interface, such as an image repository which allows users to upload images, then the lack of data verification results in an attack surface for the adversary to perform a poisoning attack.
    </p>

    <p>
      For a recent and compelling example, consider the recent release of code-generation models like OpenAI Codex<d-cite key="chen2021evaluating"></d-cite>. These tools, which translate natural language to code, are highly attractive to programmers since they promise to reduce the amount of time spent writing code. However, it is hard to guarantee the quality of the code these systems produce as output without carefully examining it. More relevant to our discussion is the source of the training data for such models, which (in the case of Codex) includes code from public code repositories. In particular, this poses a threat because an adversary can easily inject poisoned data points in the form of malicious code into public repositories, which may in turn influence the outputs of any code-generation model which learns from it.
    </p>

    <p>
      The adversary’s objective is the first consideration when crafting a poisoning attack. Poisoning attacks can be categorized as either objective-driven, where the adversary crafts the attack to satisfy a specific objective, or model-targeted, where the adversary wishes to induce a specific target model as the result of the training process. While objective-driven attacks are the most natural way to think about poisoning attacks, model-targeted attacks can make use of a general attack framework to simplify the attacker’s task to that of finding a suitable target model<d-cite key="suya2021modeltargeted"></d-cite>. We will adopt this approach for our experiments. 
    </p>

    <p>
      Most previous works have focused on two different attacker objectives: indiscriminate attacks, in which the adversary attempts to minimize the overall accuracy of the trained model<d-cite key="biggio2012poisoning, 10.5555/3007337.3007488, 10.5555/2886521.2886721, steinhardt2017certified, koh2018stronger"></d-cite>, and instance-targeted attacks, in which the adversary attempts to induce a particular behavior on a single instance<d-cite key="shafahi2018poison, zhu2019transferable, koh2017understanding, geiping2020witches, huang2020metapoison"></d-cite>. Recently, Jagielski et al. introduced the subpopulation attack, in which the adversary attempts to control the model’s behavior on a specific subpopulation while not affecting the model’s behavior on instances outside the subpopulation<d-cite key="jagielski2021subpopulation"></d-cite>.
    </p>

    <p>
      The subpopulation attack setting is particularly relevant for large datasets, since the adversary’s capacity for data injection is generally more limited when compared to the clean training data. To use the example from earlier, a targeted adversary wishing to poison a code-generation model may focus only on code generated for a specific purpose, for instance by encouraging the implementation of an encryption scheme to use an insecure mode of operation. In this example, the adversary does not necessarily wish to compromise the behavior of the model except for a particular type of input, nor does the adversary even have the necessary resources to perform such an indiscriminate attack. Instead, the adversary more carefully defines the important behavior and attempts to inject a small amount of poison data to induce it.
    </p>

    <h3>Subpopulation Susceptibility</h3>

    <p>
      A question we explore with our experiments relates to the difficulty of subpopulation poisoning attacks: what properties of a subpopulation characterize the difficulty of the attack? To answer this, we need to establish a measure of difficulty. We describe the difficulty of an attack against a dataset with $n$ datapoints which uses $p$ poisoning points as simply the ratio $p/n$. That is, the difficulty of an attack is the fraction of training points the attacker adds to the dataset. With this measure of difficulty, we can rank the attacks by difficulty and attempt to examine the properties that make attacks more or less difficult.
    </p>

    <h1>Problem Setup</h1>

    <p>
      We use the same problem setup as Suya et al. in their online model-targeted poisoning attack<d-cite key="suya2021modeltargeted"></d-cite>. In particular, we consider a binary prediction task $h : \mathcal{X} \rightarrow \mathcal{Y},$ where $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} = \{+1, -1\},$ and where the prediction model $h$ is characterized by the model parameters $\theta \in \Theta \subseteq \mathbb{R}^d.$ We denote the non-negative convex loss on a point $(x, y) \in \mathcal{X} \times \mathcal{Y}$ by $l(\theta; x, y)$, and define the loss over a set of points $A$ as $L(\theta; A) = \sum_{(x, y) \in A} l(\theta; x, y)$. Finally, we also describe the poisoning attack process with the following game-theoretic formalization:
    </p>

    <ol>
      <li>
        $N$ data points are drawn from the true data distribution over $\mathcal{X} \times \mathcal{Y}$ to produce the clean training set $\mathcal{D}_c.$
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c$ and the model space $\Theta$, generates a target classifier $\theta_p \in \Theta$ which satisfies the attack objective.
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c,$ $\Theta,$ $\theta_p,$ and the training process, produces a set of poisoning points $\mathcal{D}_p.$
      </li>
      <li>
        The model builder trains on $\mathcal{D}_c \cup \mathcal{D}_p$ to produce the induced classifier $\theta_{atk}.$
      </li>
    </ol>

    <p>
      We next describe the components we use in our experiments for each part of the above framework, as well as the formation of the subpopulations.
    </p>

    <h3>Datasets</h3>

    <p>
      We study poisoning attacks against two different types of datasets. First, we generate synthetic datasets using dataset generation algorithms from Scikit-learn <d-cite key="scikit-learn"></d-cite>. Each of these datasets is controlled by a set of parameters, which we use to control global dataset properties. The synthetic datasets are also limited to just two features, so that a direct visualization of the attack is possible. Second, we use the UCI Adult dataset <d-cite key="Dua:2019"></d-cite>, which has been used previously in evaluations of poisoning attacks <d-cite key="suya2021modeltargeted, jagielski2021subpopulation"></d-cite>. The Adult dataset is of much higher dimension (57 after data transformations), and so the attack process cannot be visualized directly as with the synthetic datasets. The purpose of this dataset is to gauge the attack behavior in a more complicated setting.
    </p>

    <h3>Subpopulation Formation and Adversary Goal</h3>

    <!-- Before performing a subpopulation attack, we have to define the subpopulation.  -->

    <p>
      We consider two subpopulation generation processes: clustering and semantic generation. Cluster subpopulations are chosen by using a k-means clustering algorithm to divide the training set into different clusters (ClusterMatch in Jagielski et al.<d-cite key="jagielski2021subpopulation"></d-cite>) and then taking only the instances with a negative label from each cluster. The attacker objective is to have the induced model misclassify the entire subpopulation as the positive label. This method is the same as used in Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>, and is useful since it allows us to measure the success of the attack as the accuracy of the induced model on the subpopulation without any ambiguity; that is, a completely successful attack attains 0% test accuracy on the target subpopulation, and in general the attack success may be measured as the error rate on the target subpopulation.
    </p>

    <p>
      In the semantic generation setting, the adversary cares about some semantic property possessed by certain instances in the dataset. Semantic subpopulations are generated by using a feature-matching algorithm (FeatureMatch in Jagielski et al.<d-cite key="jagielski2021subpopulation"></d-cite>) to obtain those instances which satisfy the semantic property and again taking only the instances with a negative label. This subpopulation generation process reflects a more realistic attacker objective, since an attacker is likely to care about influencing model behavior on a subpopulation matching a set of meaningful properties. However, this process relies on the existence of semantically meaningful features in the dataset, and so cannot be performed on our synthetic datasets.
    </p>

    <h3>Training Algorithm</h3>

    <p>
      We assume the model builder trains models using empirical risk minimization (ERM) with the following optimization strategy:
    </p>

    <d-math style="margin: auto" block>
      \theta_c = \underset{\theta \in \Theta}{\text{argmin}} \frac{1}{|\mathcal{D}_c|} L(\theta; \mathcal{D}_c) + C_r \cdot R(\theta)
    </d-math>

    <p>
      where $R(\cdot)$ is the non-negative regularization function and $C_r$ is the regularization strength parameter.
    </p>

    <h3>Target Model</h3>

    <p>
      We conduct experiments on linear SVM models. The target model for each attack was generated using the label-flipping attack variant described in Koh et al. <d-cite key="koh2018stronger"></d-cite>, which was also used by Suya et al. in their experiments<d-cite key="suya2021modeltargeted"></d-cite>. As opposed to a more general attack setting, a label-flipping attack only allows the adversary to change some fraction of the labels in the clean dataset. In the variation described by Koh et al., the adversary chooses examples from the clean dataset, flips their labels, and repeats them any number of times to produce the poisoned dataset. The target model was chosen to be the induced model resulting from this label-flipping attack. More specifically, for each subpopulation, we use the label-flipping attack to generate a collection of candidate classifiers which each achieve 0% accuracy (100% error rate) on the target subpopulation. Afterwards, the classifier with the lowest loss on the clean dataset is chosen as the target model.
    </p>

    <h3>Attack Algorithm</h3>

    <p>
      We use the online model-targeted attack algorithm developed by Suya et al<d-cite key="suya2021modeltargeted"></d-cite>. Given the clean dataset, target model, and model training parameters, the algorithm produces a set of poisoning points by maintaining an intermediate induced model and choosing points which maximize the loss difference between the intermediate model and target model. As each poisoning point is chosen, the intermediate model is updated using a new set of points (now including the newly chosen poison point) using the knowledge of the model training process. Importantly, the online attack provides theoretical guarantees about convergence of the induced model to the target model as the number of poisoning points increases. Since we have chosen our target model to misclassify the entire subpopulation, this means we have a guarantee that the online attack process will eventually produce a poisoned dataset which induces a satisfactory target model.
    </p>

    <p>
      A useful consequence of using the online attack algorithm is that the rate of convergence is characterized by the loss difference between the target and clean models on the clean dataset. If we define the <em>loss-based distance</em> $D_{l, \mathcal{X}, \mathcal{Y}} : \Theta \times \Theta \rightarrow \mathbb{R}$ between two models $\theta_1, \theta_2$ over a space $\mathcal{X} \times \mathcal{Y}$ with loss function $l(\theta; x, y)$ by
    </p>

    <d-math style="margin: auto" block>
       D_{l, \mathcal{X}, \mathcal{Y}}(\theta_1, \theta_2) := \max_{(x, y) \in \mathcal{X} \times \mathcal{Y}} l(\theta_1; x, y) - l(\theta_2; x, y),
    </d-math>

    <p>
      then the loss-based distance between the induced model $\theta_{atk}$ and the target model $\theta_p$ correlates to the loss difference $L(\theta_p; \mathcal{D}_c) - L(\theta_c; D_c)$ between $\theta_p$ and the clean model $\theta_c$ on the clean dataset. This fact implies a general heuristic for predicting attack difficulty: the closer a target model is to the clean model as measured by loss difference on the clean dataset, the easier the attack will be. This also justifies the decision to choose a target model with lower loss on the clean dataset.
    </p>

    <p>
      Now that we have outlined the attack process, we can move on to our first set of experiments on synthetic datasets.
    </p>

    <h1>Synthetic Datasets</h1>

    <figure style="text-align: center" id="demo2">
      <svg height="448px" width="448px">
        <image xlink:href="images/datasets.png" height="448" width="448"></image>
      </svg>
      <figcaption>
        We generate several synthetic datasets controlled by two parameters: a <em>class separation</em> parameter, which controls the distance between class centers, and a <em>label flip</em> parameter, which controls the amount of label noise present in the dataset.
      </figcaption>
    </figure>

    <h3>Synthetic Dataset Generation</h3>

    <p>
      For the first set of experiments, we consider a family of synthetic datasets resembling Gaussian mixtures with two components. The datasets are controlled by two parameters, which we will refer to as the dataset parameters. The first dataset parameter is a class separation parameter $\alpha \ge 0$ which controls the distance between class centers. The second dataset parameter is a label noise parameter $\beta \in [0, 1]$ which controls the fraction of points whose labels are randomly assigned. Fixing the dataset parameters, we generate several datasets using different random seeds. Each of the datasets is generated with $n=3000$ points and is class-balanced, before being divided into train and test sets in a 2:1 ratio.
    </p>

    <h3>Dataset Parameter Space</h3>

    <p>
      We generate the datasets over a grid of the dataset parameters. For each combination of parameters, ten synthetic datasets are created using different random seeds. Before attacking the subpopulations, let’s observe the behavior of the clean models trained on each combination of the dataset parameters.
    </p>

    <figure style="text-align: center" id="dataset-params">
      <svg height="448px" width="448px">
        <image xlink:href="images/dataset-params.png" height="448" width="448"></image>
      </svg>
      <figcaption>The clean model performance is affected by the dataset parameters: as the classes become more separated and less noisy, the classifier is able to better separate them. Hover over points on the plot to see the datasets generated with each parameter combination. (Ideally, this figure would be interactive: hovering over points on the plot could reveal the actual datasets generated with those parameters.)</figcaption>
    </figure>

    <p>
      The overall trends in the plot make intuitive sense: clean model accuracy improves as the classes become more separated and exhibit less label noise.
    </p>

    <p>
      On the left and upper edges of the plot, we see that the clean model performs very poorly on datasets with either high label noise or small class separation. One hypothesis we might develop is that attacks against subpopulations within these datasets are easy: since the loss difference between any two models will be close to zero, the attack should be effective with only a few poison points. On the other hand, datasets with accurate clean models should produce subpopulations that are harder to attack, since the loss difference between an accurate model and an inaccurate one is high.
    </p>

    <h3>Attack Visualization</h3>

    <p>
      We can now move onto visualizing the poisoning attacks on these datasets. For each synthetic dataset, we run the k-means clustering algorithm (k=16) and extract the negative-label instances from each cluster to form the subpopulations. We then generate target models for each nonempty subpopulation according to the earlier specification (100% error rate on the target subpopulation, measured on the test set). Finally, we use the online attack to generate a set of poisons which induce the desired behavior. The attack completes when the induced model misclassifies at least 50% of the target subpopulation, measured on the training set.
    </p>

    <figure style="text-align: center" id="anim1">
      <video width="448" height="448" controls>
        <source src="animations/anim1.mp4" type="video/mp4">
      </video>
      <figcaption>
        The poisons are added sequentially to induce a model which misclassifies the orange points as the positive label.
      </figcaption>
    </figure>

    <p>
      The above animation shows how the online attack sequntially adds points into the dataset to move the induced model towards the target model. Take note of how the positive-labeled poisons work to reorient the model's decision boundary to cover the subpopulation while minimizing the impact on the rest of the dataset. This behavior reflects the part of the target model selection criteria which minimizes the loss on the clean dataset. Another possibility would have been to push the entire decision boundary downwards without reorienting it<d-footnote>i.e., to use a target model which only differs from the clean model in the bias term.</d-footnote>, but this would have resulted in a model with higher loss on other parts of the dataset, and thus a higher loss difference to the clean model. Intuitively, such an alternative would experience more "resistance" from the dataset, preventing the induced model from moving as swiftly to the target.
    </p>

    <!-- <p>
      As for how the individual points are chosen, we can also see that the 
    </p> -->

    <p>
      With the ability to visualize poisoning attacks, we are now equipped to analyze some of the subpopulation properties that determine attack difficulty.
    </p>

    <figure style="text-align: center" id="anim2a">
      <video width="448" height="448" controls>
        <source src="animations/anim2a.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack against a dataset with high label noise is easy, since the clean model is not heavily influenced by the points already in the training set.
      </figcaption>
    </figure>

    <p>
      Let's start with a low-difficulty attack. The above dataset exhibits a large amount of label noise, and as a result the clean model performs very poorly. When poisoning points are added, the model is quickly persuaded to change its decision to correctly classify the target subpopulation. Observe that the poisoned model labels all points in the dataset as the positive label, with the decision boundary lying out of view. In other words, the attack is changing not just the model behavior local to the target subpopulation, but also the global behavior of the model.
    </p>

    <!-- <figure style="text-align: center" id="anim2b">
      <video width="448" height="448" controls>
        <source src="animations/anim2b.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack against a dataset with high label noise is easy, since the clean model is not heavily influenced by the points already in the training set.
      </figcaption>
    </figure>

    <p>
      If we lower the label noise slightly, the attack does not induce quite as strong a response from the intermediate models,
    </p> -->

    <figure style="text-align: center" id="anim3">
      <video width="448" height="448" controls>
        <source src="animations/anim3.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack against a dataset with close class centers is easy, since the linear SVM model does not have sufficient capacity to perform well on the clean dataset.
      </figcaption>
    </figure>

    <p>
      The next example shows a situation similar to the noisy dataset, but now for a slightly different reason: although the data now form meaningful classes, the model does not have enough capacity to create a useful classification function. The result is the same: poison points strongly influence the behavior of the model. Note that in both of the above examples, it was important that the classes be balanced; if the labels were represented in a different ratio, the clean model would have preferred to classify all points as the dominant label, and the attack may not have been as easy.
    </p>

    <p>
      As mentioned earlier, the properties that apply to the above two datasets should apply to all the datasets with either close class centers or high label noise. We can demonstrate this empirically by looking at the mean attack difficulty over the entire grid of dataset parameters:
    </p>

    <figure style="text-align: center" id="difficulty-vs-params">
      <svg height="448px" width="448px">
        <image xlink:href="images/dataset-difficulty.png" height="448" width="448"></image>
      </svg>
      <figcaption>
        Mean attack difficulty appears to be a function of the clean model accuracy. (Again, an interactive plot would be preferable)
      </figcaption>
    </figure>

    <p>
      It appears that the attacks against the datasets with poor clean models tend to be the easiest, and in general attack difficulty increases with clean model accuracy. The behavior is the most consistent with low accuracy clean models, since all attacks are easy and require few poisons. As the clean model accuracy increases, the behvaior becomes more complex and attack difficulty begins to depend on the properties of the subpopulation. Since attack difficulty is driven by the loss difference between the target and clean models, it makes sense to try to characterize the behavior of further examples in terms of this quantity.
    </p>

    <figure style="text-align: center" id="anim4">
      <video width="448" height="448" controls>
        <source src="animations/anim4.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack with low collateral damage is easy, since the loss difference between the target and clean models is small.
      </figcaption>
    </figure>

    <p>
      The next attack is easy for the attacker despite being against a dataset with an accurate clean model. The targeted subpopulation lies on the boundary of the class it belongs to, and more specifically the loss difference between the target and clean models is small. In other words, the attack causes very little "collateral damage."
    </p>

    <!-- <p>
      This example possesses a few interesting properties, in addition to its low difficulty. First, the clean and induced models have decision boundaries with significantly different orientations. 
    </p> -->

    <figure style="text-align: center" id="anim5">
      <video width="448" height="448" controls>
        <source src="animations/anim5.mp4" type="video/mp4">
      </video>
      <figcaption>
        Caption
      </figcaption>
    </figure>

    <p>
      This is our first example of an exceptionally difficult attack, requiring over 800 poisons. The loss difference between the target and clean models is high, since the target model misclassifies a large number of points in the negative class. Notice that it seems hard to even produce a target model which does not cause a large amount of collateral damage. In some sense, the subpopulation is protected against attacks due it its central location: 
    </p>

    <p>
      An interesting property of this attack is that the decision boundary seems to move very slowly until the end of the attack, at which point the decision boundary becomes unstable and jumps around before landing in a position which completes the attack. The model even appears to run into some convergence issues near the end, such as on the 823rd and 837th inserted poisons. An interesting experiment would be to examine how the loss over the entire model parameter space changes as poisons are added, which might explain the instability and convergence issues.
    </p>

    <h3>Quantitative Analysis</h3>
    
    <p>
      Visualizations are helpful, but are rarely possible given the high-dimensional nature of most datasets. Now that we have visualized some of the attacks in the lower-dimensional setting, can we quantify the properties that describe the difficulty of poisoning attacks?
    </p>

    <p>
      Earlier we made the obseravation that attack difficulty tends to increase with clean model accuracy. To be a little more precise, we can see how the attack difficulty distribution changes as a function of clean model accuracy, for our attack setup.
    </p>

    <figure style="text-align: center" id="difficulty-accuracy">
      <svg height="336px" width="672px">
        <image xlink:href="images/difficulty-accuracy.png" height="336" width="336"></image>
        <image xlink:href="images/difficulty-slice.png" x="336" height="336" width="336"></image>
      </svg>
      <figcaption>
        My intention for this figure is for the reader to be able to adjust a window on the left to change the histogram on the right, probably removing the lines unless they are explainable in some interesting way.
      </figcaption>
    </figure>

    <!-- <p>
      For a fixed clean model accuracy $\alpha$, the difficulty distribution 
    </p> -->

    <p>
      (It would be nice to have a paragraph or two explaining the above phenomenon more theoretically, but I haven't thought much about the details)
    </p>

    <h3>Aside: Target Model Interpolation?</h3>

    <p>
      (I'm not sure if this discussion fits into this article, maybe best to leave out)
    </p>

    <h1>Adult Dataset</h1>

    <p>
      (Not yet written)
    </p>

    <h1>Discussion</h1>

    <p>
      A natural objection to this analysis is that the linear SVM is not a sufficiently complex model to produce meaningful results about subpopulation susceptibility. We offer two rebuttals: the first is that simple, low-capacity models are still used widely in practice due to their ease of use, low computational cost, and effectiveness<d-cite key="decrema2019progress, tramer2021differentially"></d-cite>, and so our simplified analysis is still relevant in practice. The second is that models may use kernel methods to first project input data into a higher-dimensional space and then use a hyperplane to classify the transformed input. Assuming our observations about the importance of spatial relationships within datasets hold in higher dimensional spaces, the same conclusions reached in our simplified setting still apply to the more complex setting by examining the spatial relationships in the transformed space.
    </p>

    <p>
      Another objection is that the model-targeted attack may not adequately describe the difficulty of any attack against a subpopulation, since a more optimal attack may exist using a different target model or different attack framework.
    </p>

    <p>
      There is also the fact that we studied these attacks in the absence of any defenses.
    </p>

    <h1>Conclusion</h1>

    <p>
      (Not yet written)
    </p>

  </d-article>



  <d-appendix>
    <!-- <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to...
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>