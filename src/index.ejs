<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%= require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Poisoning Attacks and Subpopulation Susceptibility</h1>
    <p>Experiments on subpopulations vulnerable to poisoning attacks.</p>
  </d-title>

  <figure style="position: relative; margin: auto; width: 984px; height: 425px">
    <div id="demo" style="position:relative; height:300px; border: 1px solid rgba(0, 0, 0, 0.2); background-color: rgba(0, 0, 0, 0.2); text-align: center">Interactive attack demo: the reader can generate datasets using the same parameters used in the experiments, create subpopulations, and try to manually add poisons to achieve the objective</div>
    <svg id="actualdemo"></svg>
    <div id="controls" style="position: absolute; width: 300px; height: 50px; left: 20px; top: 320px">
      <div><p id="slider-value"></p></div>
      <div><div id="slider"></div></div>
      <!-- <text class="figtext" style="top: -5px; left: 20px; position: relative;">Demo Controls here?</text> -->
    </div>
    <!-- <div id="sliderAlpha" style="position: absolute; width: 300px; height: 50px; left: 20px; top: 320px">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Class Separation $d = 1.0$</text>
    </div>
    <div id="sliderBeta" style="position:absolute; width: 300px; height: 50px; left: 280px; top: 320px;;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Label Noise $p = 0.2$</text>
    </div> -->
    <figcaption id="democaption" style="position:absolute; width: 500px; top: 320px; left: 484px">
      Machine learning is susceptible to poisoning attacks where adversaries inject poisoned training data into the training set, usually to induce specific model behavior. When attackers focus their efforts towards subpopulations of the data distribution, which subpopulations are the most vulnerable to an attack?
    </figcaption>
  </figure>

  <d-article>
    <p>
      Machine learning is susceptible to poisoning attacks, where an attacker controls a small fraction of the training data and chooses that data to induce some undesired behavior in the trained model<d-cite key="10.5555/1387709.1387716, biggio2012poisoning"></d-cite>. Previous works have mostly considered two attacker objectives: indiscriminate attacks, where the attacker's goal is to reduce overall model accuracy<d-cite key="biggio2012poisoning, 10.5555/3007337.3007488, 10.5555/2886521.2886721, steinhardt2017certified, koh2018stronger"></d-cite>, and instance-targeted attacks, where the attacker's goal is to reduce accuracy on a specific instance<d-cite key="shafahi2018poison, zhu2019transferable, koh2017understanding, geiping2020witches, huang2020metapoison"></d-cite>. Recently, <d-cite key="jagielski2021subpopulation"></d-cite> introduced the subpopulation attack, a more realistic setting in which the adversary attempts to control the model’s behavior on a specific subpopulation while not affecting the model’s behavior on instances outside the subpopulation. In this article, we explore a question about poisoning attacks against subpopulations of a data distribution: how do subpopulation characteristics affect attack difficulty? We will attempt to visually understand these attacks by animating a poisoning attack algorithm in a simplified setting, and additionally try to describe the difficulty of the attacks in terms of the properties of the subpopulations they are against.
    </p>

    <h1>Problem Setup</h1>

    <p>
      We use the same problem setup as Suya et al. in their online model-targeted poisoning attack<d-cite key="suya2021modeltargeted"></d-cite>. In particular, we consider a binary prediction task $h : \mathcal{X} \rightarrow \mathcal{Y},$ where $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} = \{+1, -1\},$ and where the prediction model $h$ is characterized by the model parameters $\theta \in \Theta \subseteq \mathbb{R}^d.$ We denote the non-negative convex loss on a point $(x, y) \in \mathcal{X} \times \mathcal{Y}$ by $l(\theta; x, y)$, and define the loss over a set of points $A$ as $L(\theta; A) = \sum_{(x, y) \in A} l(\theta; x, y)$. Finally, we also describe the poisoning attack process with the following game-theoretic formalization:
    </p>

    <ol>
      <li>
        $N$ data points are drawn from the true data distribution over $\mathcal{X} \times \mathcal{Y}$ to produce the clean training set $\mathcal{D}_c.$
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c$ and the model space $\Theta$, generates a target classifier $\theta_p \in \Theta$ which satisfies the attack objective.
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c,$ $\Theta,$ $\theta_p,$ and the training process, produces a set of poisoning points $\mathcal{D}_p.$
      </li>
      <li>
        The model builder trains on $\mathcal{D}_c \cup \mathcal{D}_p$ to produce the induced classifier $\theta_{atk}.$
      </li>
    </ol>

    <p>
      We next describe the components we use in our experiments for each part of the above framework, as well as the formation of the subpopulations.
    </p>

    <h3>Datasets</h3>

    <p>
      We study poisoning attacks against two different types of datasets. First, we generate synthetic datasets using dataset generation algorithms from Scikit-learn <d-cite key="scikit-learn"></d-cite>. Each of these datasets is controlled by a set of parameters, which we use to control global dataset properties. The synthetic datasets are also limited to just two features, so that a direct visualization of the attack is possible. Second, we use the UCI Adult dataset <d-cite key="Dua:2019"></d-cite>, which has been used previously in evaluations of poisoning attacks <d-cite key="suya2021modeltargeted, jagielski2021subpopulation"></d-cite>. The Adult dataset is of much higher dimension (57 after data transformations), and so the attack process cannot be visualized directly as with the synthetic datasets. The purpose of this dataset is to gauge the attack behavior in a more complicated setting.
    </p>

    <h3>Subpopulation Formation and Adversary Goal</h3>

    <p>
      We consider two subpopulation generation processes: clustering and semantic generation. Cluster subpopulations are chosen by using a k-means clustering algorithm to divide the training set into different clusters (ClusterMatch in Jagielski et al.<d-cite key="jagielski2021subpopulation"></d-cite>) and then taking only the instances with a negative label from each cluster. The attacker objective is to have the induced model misclassify the entire subpopulation as the positive label. This method is the same as used in Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>, and is useful since it allows us to measure the success of the attack as the accuracy of the induced model on the subpopulation without any ambiguity; that is, a completely successful attack attains 0% test accuracy on the target subpopulation, and in general the attack success may be measured as the error rate on the target subpopulation.
    </p>

    <p>
      In the semantic generation setting, the adversary cares about some semantic property possessed by certain instances in the dataset. Semantic subpopulations are generated by using a feature-matching algorithm (FeatureMatch in Jagielski et al.<d-cite key="jagielski2021subpopulation"></d-cite>) to obtain those instances which satisfy the semantic property and again taking only the instances with a negative label. This subpopulation generation process reflects a more realistic attacker objective, since an attacker is likely to care about influencing model behavior on a subpopulation matching a set of meaningful properties. However, this process relies on the existence of semantically meaningful features in the dataset, and so cannot be performed on our synthetic datasets.
    </p>

    <h3>Training Algorithm</h3>

    <p>
      We assume the model builder trains models using empirical risk minimization (ERM) with the following optimization strategy:
    </p>

    <d-math style="margin: auto" block>
      \theta_c = \underset{\theta \in \Theta}{\text{argmin}} \frac{1}{|\mathcal{D}_c|} L(\theta; \mathcal{D}_c) + C_r \cdot R(\theta)
    </d-math>

    <p>
      where $R(\cdot)$ is the non-negative regularization function and $C_r$ is the regularization strength parameter.
    </p>

    <h3>Target Model</h3>

    <p>
      We conduct experiments on linear SVM models. The target model for each attack was generated using the label-flipping attack variant described in Koh et al. <d-cite key="koh2018stronger"></d-cite>, which was also used by Suya et al. in their experiments<d-cite key="suya2021modeltargeted"></d-cite>. As opposed to a more general attack setting, a label-flipping attack only allows the adversary to change some fraction of the labels in the clean dataset. In the variation described by Koh et al., the adversary chooses examples from the clean dataset, flips their labels, and repeats them any number of times to produce the poisoned dataset. The target model was chosen to be the induced model resulting from this label-flipping attack. More specifically, for each subpopulation, we use the label-flipping attack to generate a collection of candidate classifiers which each achieve 0% accuracy (100% error rate) on the target subpopulation. Afterwards, the classifier with the lowest loss on the clean dataset is chosen as the target model.
    </p>

    <h3>Attack Algorithm</h3>

    <p>
      We use the online model-targeted attack algorithm developed by Suya et al<d-cite key="suya2021modeltargeted"></d-cite>. Given the clean dataset, target model, and model training parameters, the algorithm produces a set of poisoning points by maintaining an intermediate induced model and choosing points which maximize the loss difference between the intermediate model and target model. As each poisoning point is chosen, the intermediate model is updated using a new set of points (now including the newly chosen poison point) using the knowledge of the model training process. Importantly, the online attack provides theoretical guarantees about convergence of the induced model to the target model as the number of poisoning points increases. Since we have chosen our target model to misclassify the entire subpopulation, this means we have a guarantee that the online attack process will eventually produce a poisoned dataset which induces a satisfactory target model.
    </p>

    <p>
      A useful consequence of using the online attack algorithm is that the rate of convergence is characterized by the loss difference between the target and clean models on the clean dataset. If we define the <em>loss-based distance</em> $D_{l, \mathcal{X}, \mathcal{Y}} : \Theta \times \Theta \rightarrow \mathbb{R}$ between two models $\theta_1, \theta_2$ over a space $\mathcal{X} \times \mathcal{Y}$ with loss function $l(\theta; x, y)$ by
    </p>

    <d-math style="margin: auto" block>
       D_{l, \mathcal{X}, \mathcal{Y}}(\theta_1, \theta_2) := \max_{(x, y) \in \mathcal{X} \times \mathcal{Y}} l(\theta_1; x, y) - l(\theta_2; x, y),
    </d-math>

    <p>
      then the loss-based distance between the induced model $\theta_{atk}$ and the target model $\theta_p$ correlates to the loss difference $L(\theta_p; \mathcal{D}_c) - L(\theta_c; D_c)$ between $\theta_p$ and the clean model $\theta_c$ on the clean dataset. This fact implies a general heuristic for predicting attack difficulty: the closer a target model is to the clean model as measured by loss difference on the clean dataset, the easier the attack will be. This also justifies the decision to choose a target model with lower loss on the clean dataset.
    </p>

    <p>
      Now that we have outlined the attack process, we can move on to our first set of experiments on synthetic datasets.
    </p>

    <h1>Synthetic Datasets</h1>

    <figure style="text-align: center" id="demo2">
      <svg height="448px" width="448px">
        <image xlink:href="images/datasets.png" height="448" width="448"></image>
      </svg>
      <figcaption>
        We generate several synthetic datasets controlled by two parameters: a <em>class separation</em> parameter, which controls the distance between class centers, and a <em>label flip</em> parameter, which controls the amount of label noise present in the dataset.
      </figcaption>
    </figure>

    <!-- <d-figure id="svelte-scatterplot-dfigure-example1" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
    </d-figure>

    <d-figure id="svelte-scatterplot-dfigure-example2" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
    </d-figure> -->

    <h3>Synthetic Dataset Generation</h3>

    <p>
      For the first set of experiments, we consider a family of synthetic datasets resembling Gaussian mixtures with two components. The datasets are controlled by two parameters, which we will refer to as the dataset parameters. The first dataset parameter is a class separation parameter $\alpha \ge 0$ which controls the distance between class centers. The second dataset parameter is a label noise parameter $\beta \in [0, 1]$ which controls the fraction of points whose labels are randomly assigned. Fixing the dataset parameters, we generate several datasets using different random seeds. Each of the datasets is generated with $n=3000$ points and is class-balanced, before being divided into train and test sets in a 2:1 ratio.
    </p>

    <h3>Dataset Parameter Space</h3>

    <p>
      We generate the datasets over a grid of the dataset parameters. For each combination of parameters, ten synthetic datasets are created using different random seeds. Before attacking the subpopulations, let’s observe the behavior of the clean models trained on each combination of the dataset parameters.
    </p>

    <figure style="text-align: center" id="dataset-params">
      <svg height="448px" width="448px">
        <image xlink:href="images/dataset-params.png" height="448" width="448"></image>
      </svg>
      <figcaption>The clean model performance is affected by the dataset parameters: as the classes become more separated and less noisy, the classifier is able to better separate them. Hover over points on the plot to see the datasets generated with each parameter combination. (Ideally, this figure would be interactive: hovering over points on the plot could reveal the actual datasets generated with those parameters.)</figcaption>
    </figure>

    <p>
      The overall trends in the plot make intuitive sense: clean model accuracy improves as the classes become more separated and exhibit less label noise.
    </p>

    <p>
      On the left and upper edges of the plot, we see that the clean model performs very poorly on datasets with either high label noise or small class separation. One hypothesis we might develop is that attacks against subpopulations within these datasets are easy: since the loss difference between any two models will be close to zero, the attack should be effective with only a few poison points. On the other hand, datasets with accurate clean models should produce subpopulations that are harder to attack, since the loss difference between an accurate model and an inaccurate one is high.
    </p>

    <h3>Attack Visualization</h3>

    <p>
      We can now move onto visualizing the poisoning attacks on these datasets. For each synthetic dataset, we run the k-means clustering algorithm (k=16) and extract the negative-label instances from each cluster to form the subpopulations. We then generate target models for each nonempty subpopulation according to the earlier specification (100% error rate on the target subpopulation, measured on the test set). Finally, we use the online attack to generate a set of poisons which induce the desired behavior. The attack completes when the induced model misclassifies at least 50% of the target subpopulation, measured on the training set.
    </p>

    <p>
      We describe the difficulty of an attack against a dataset with $n$ datapoints which uses $p$ poisoning points as simply the ratio $p/n$. That is, the difficulty of an attack is the fraction of training points the attacker adds to the dataset. With this measure of difficulty, we can rank the attacks by difficulty and attempt to examine the properties that make attacks more or less difficult.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example1" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>The poisons are added sequentially to induce a model which misclassifies the orange points as the positive label.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim1">
      <video width="448" height="448" controls>
        <source src="animations/anim1.mp4" type="video/mp4">
      </video>
      <figcaption>
        The poisons are added sequentially to induce a model which misclassifies the orange points as the positive label.
      </figcaption>
    </figure> -->

    <p>
      The above animation shows how the online attack sequntially adds points into the dataset to move the induced model towards the target model. Take note of how the positive-labeled poisons work to reorient the model's decision boundary to cover the subpopulation while minimizing the impact on the rest of the dataset. This behavior reflects the part of the target model selection criteria which minimizes the loss on the clean dataset. Another possibility would have been to push the entire decision boundary downwards without reorienting it<d-footnote>i.e., to use a target model which only differs from the clean model in the bias term.</d-footnote>, but this would have resulted in a model with higher loss on other parts of the dataset, and thus a higher loss difference to the clean model. Intuitively, such an alternative would experience more "resistance" from the dataset, preventing the induced model from moving as swiftly to the target.
    </p>

    <!-- <p>
      As for how the individual points are chosen, we can also see that the 
    </p> -->

    <p>
      With the ability to visualize poisoning attacks, we are now equipped to analyze some of the subpopulation properties that determine attack difficulty.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example2" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against a dataset with high label noise is easy, since the clean model is not heavily influenced by the points already in the training set.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim2a">
      <video width="448" height="448" controls>
        <source src="animations/anim2a.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack against a dataset with high label noise is easy, since the clean model is not heavily influenced by the points already in the training set.
      </figcaption>
    </figure> -->

    <p>
      Let's start with a low-difficulty attack. The above dataset exhibits a large amount of label noise, and as a result the clean model performs very poorly. When poisoning points are added, the model is quickly persuaded to change its decision to correctly classify the target subpopulation. Observe that the poisoned model labels all points in the dataset as the positive label, with the decision boundary lying out of view. In other words, the attack is changing not just the model behavior local to the target subpopulation, but also the global behavior of the model.
    </p>

    <!-- <figure style="text-align: center" id="anim2b">
      <video width="448" height="448" controls>
        <source src="animations/anim2b.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack against a dataset with high label noise is easy, since the clean model is not heavily influenced by the points already in the training set.
      </figcaption>
    </figure>

    <p>
      If we lower the label noise slightly, the attack does not induce quite as strong a response from the intermediate models,
    </p> -->

    <d-figure id="svelte-scatterplot-dfigure-example3" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against a dataset with close class centers is easy, since the linear SVM model does not have sufficent capacity to perform well on the clean dataset.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim3">
      <video width="448" height="448" controls>
        <source src="animations/anim3.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack against a dataset with close class centers is easy, since the linear SVM model does not have sufficient capacity to perform well on the clean dataset.
      </figcaption>
    </figure> -->

    <p>
      The next example shows a situation similar to the noisy dataset, but now for a slightly different reason: although the data now form meaningful classes, the model does not have enough capacity to create a useful classification function. The result is the same: poison points strongly influence the behavior of the model. Note that in both of the above examples, it was important that the classes be balanced; if the labels were represented in a different ratio, the clean model would have preferred to classify all points as the dominant label, and the attack may not have been as easy.
    </p>

    <p>
      As mentioned earlier, the properties that apply to the above two datasets should apply to all the datasets with either close class centers or high label noise. We can demonstrate this empirically by looking at the mean attack difficulty over the entire grid of dataset parameters:
    </p>

    <figure style="text-align: center" id="difficulty-vs-params">
      <svg height="448px" width="448px">
        <image xlink:href="images/dataset-difficulty.png" height="448" width="448"></image>
      </svg>
      <figcaption>
        Mean attack difficulty appears to be a function of the clean model accuracy. (Again, an interactive plot would be preferable)
      </figcaption>
    </figure>

    <p>
      It appears that the attacks against the datasets with poor clean models tend to be the easiest, and in general attack difficulty increases with clean model accuracy. The behavior is the most consistent with low accuracy clean models, since all attacks are easy and require few poisons. As the clean model accuracy increases, the behvaior becomes more complex and attack difficulty begins to depend on the properties of the subpopulation. Since attack difficulty is driven by the loss difference between the target and clean models, it makes sense to try to characterize the behavior of further examples in terms of this quantity.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example4" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack with low collateral damage is easy, since the loss difference between the target and clean models is small.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim4">
      <video width="448" height="448" controls>
        <source src="animations/anim4.mp4" type="video/mp4">
      </video>
      <figcaption>
        An attack with low collateral damage is easy, since the loss difference between the target and clean models is small.
      </figcaption>
    </figure> -->

    <p>
      The next attack is easy for the attacker despite being against a dataset with an accurate clean model. The targeted subpopulation lies on the boundary of the class it belongs to, and more specifically the loss difference between the target and clean models is small. In other words, the attack causes very little "collateral damage."
    </p>

    <!-- <p>
      This example possesses a few interesting properties, in addition to its low difficulty. First, the clean and induced models have decision boundaries with significantly different orientations. 
    </p> -->

    <d-figure id="svelte-scatterplot-dfigure-example5" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against the "center" of a cluster is hard, since the subpopulation is "protected" by surrounding data.</figcaption>
    </d-figure>

    <!-- <figure style="text-align: center" id="anim5">
      <video width="448" height="448" controls>
        <source src="animations/anim5.mp4" type="video/mp4">
      </video>
      <figcaption>
        Caption
      </figcaption>
    </figure> -->

    <p>
      This is our first example of an exceptionally difficult attack, requiring over 800 poisons. The loss difference between the target and clean models is high, since the target model misclassifies a large number of points in the negative class. Notice that it seems hard to even produce a target model which does not cause a large amount of collateral damage. In some sense, the subpopulation is protected against attacks due it its central location: 
    </p>

    <p>
      An interesting property of this attack is that the decision boundary seems to move very slowly until the end of the attack, at which point the decision boundary becomes unstable and jumps around before landing in a position which completes the attack. The model even appears to run into some convergence issues near the end, such as on the 823rd and 837th inserted poisons. An interesting experiment would be to examine how the loss over the entire model parameter space changes as poisons are added, which might explain the instability and convergence issues.
    </p>

    <h3>Quantitative Analysis</h3>
    
    <p>
      Visualizations are helpful, but are rarely possible given the high-dimensional nature of most datasets. Now that we have visualized some of the attacks in the lower-dimensional setting, can we quantify the properties that describe the difficulty of poisoning attacks?
    </p>

    <p>
      Earlier we made the obseravation that attack difficulty tends to increase with clean model accuracy. To be a little more precise, we can see how the attack difficulty distribution changes as a function of clean model accuracy, for our attack setup.
    </p>

    <figure style="text-align: center" id="difficulty-accuracy">
      <svg height="336px" width="672px">
        <image xlink:href="images/difficulty-accuracy.png" height="336" width="336"></image>
        <image xlink:href="images/difficulty-slice.png" x="336" height="336" width="336"></image>
      </svg>
      <figcaption>
        My intention for this figure is for the reader to be able to adjust a window on the left to change the histogram on the right, probably removing the lines unless they are explainable in some interesting way.
      </figcaption>
    </figure>

    <!-- <p>
      For a fixed clean model accuracy $\alpha$, the difficulty distribution 
    </p> -->

    <p>
      (It would be nice to have a paragraph or two explaining the above phenomenon more theoretically, but I haven't thought much about the details)
    </p>

    <h3>Aside: Target Model Interpolation?</h3>

    <p>
      (I'm not sure if this discussion fits into this article, maybe best to leave out)
    </p>

    <h1>Adult Dataset</h1>

    <p>
      (Not yet written)
    </p>

    <h1>Discussion</h1>

    <p>
      A natural objection to this analysis is that the linear SVM is not a sufficiently complex model to produce meaningful results about subpopulation susceptibility. We offer two rebuttals: the first is that simple, low-capacity models are still used widely in practice due to their ease of use, low computational cost, and effectiveness<d-cite key="decrema2019progress, tramer2021differentially"></d-cite>, and so our simplified analysis is still relevant in practice. The second is that models may use kernel methods to first project input data into a higher-dimensional space and then use a hyperplane to classify the transformed input. Assuming our observations about the importance of spatial relationships within datasets hold in higher dimensional spaces, the same conclusions reached in our simplified setting still apply to the more complex setting by examining the spatial relationships in the transformed space.
    </p>

    <p>
      Another objection is that the model-targeted attack may not adequately describe the difficulty of any attack against a subpopulation, since a more optimal attack may exist using a different target model or different attack framework.
    </p>

    <p>
      There is also the fact that we studied these attacks in the absence of any defenses.
    </p>

    <h1>Conclusion</h1>

    <p>
      (Not yet written)
    </p>

  </d-article>



  <d-appendix>
    <!-- <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to...
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>