<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Poisoning Attacks and Subpopulation Susceptibility</h1>
    <p>An Experimental Exploration on the Effectiveness of Poisoning Attacks</p>
  </d-title>

  <d-figure id="svelte-poison-demo-dfigure" class="poison-demo">
    <figure>
      <div id="svelte-demo-target" style="text-align: center"></div>
      <div id="svelte-demo-controls" class="poison-demo-controls">
        <div id="alpha-container" style="top:0px;" class="slider-container">
          <label id="alphaText" for="alphaSlider" class="poison-demo-controls-text">Class Separation α = 1.00</label>
          <input id="alphaSlider" type="range" class="slider control-slider" min="0" max="12" value="4" />
        </div>
        <div id="beta-container" style="top:54px;" class="slider-container">
          <label id="betaText" for="betaSlider" class="poison-demo-controls-text">Random Label Fraction β = 0.00</label>
          <input id="betaSlider" type="range" class="slider control-slider" min="0" max="10" value="0" />
        </div>
        <div id="seed-container" style="top:116px;" class="seed-container">
          <input id="seedButton" type="button" class="button seed-button" value="Randomize Dataset" />
          <label for="seedField" class="seed-label">Dataset seed</label>
          <input id="seedField" type="number" class="input seed-textfield" value="1" />
        </div>
        <div style="position:absolute; right:188px; height: 100%; width:1.5px; background-color:#808080;"></div>
        <div id="mode-container" class="mode-container">
          <label for="mode" class="mode-label">Poisoning algorithm:</label>
          <select style="position:absolute; left:140px; top:4px" id="attackAlgo" name="Attack Algorithm">
            <option value="manual">Manual</option>
            <option value="labelFlip">Label flip</option>
            <option value="modelTargeted">Model-targeted</option>
          </select>
        </div>
        <div id="manual-buttons" class="demo-buttons-container">
          <button id="labelButton" class="button control-button">
            <div>
              Label:
            </div>
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M0 5Q0 10 5 10 10 10 10 5 10 0 5 0 0 0 0 5" fill="steelblue" />
            </svg>
          </button>
          <button id="toolButton" class="button control-button">
            <div>
              Tool:
            </div>
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.2145 7.1365-.0113 9.9037 2.7559 8.678l-1.5415-1.5415zM1.5838 6.6299 7.0865 1.1336l1.6781 1.6799L3.2617 8.3098zM9.2027 2.3757l.3494-.3493A1.1871 1.1871 90 107.8736.3477L7.5238.6961z" fill="#1f1f1f" />
            </svg>
          </button>
          <button id="resetButton" class="button control-button">
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.5 2 1.5 10 8.5 10 8.5 2 7.5 2 7.5 9 2.5 9 2.5 2ZM0.5 1 9.5 1 9.5 2 0.5 2ZM4 1 6 1 6 0 6 0 4 0ZM3 3 3 8 4 8 4 3ZM6 3 6 8 7 8 7 3ZM4.5 3 4.5 8 5.5 8 5.5 3Z" fill="#1f1f1f" />
            </svg>
          </button>
        </div>
        <div id="algorithm-buttons" class="demo-buttons-container" style="visibility: hidden;">
          <button id="algorithmPlayButton" class="button control-button" style="cursor: pointer">
            <svg width="10" height="10" viewBox="0 0 10 10">
              <path d="M0 0L0 10L3 10L3 0ZM6 0L6 10L9 10L9 0Z" ; fill="#1f1f1f" />
            </svg>
          </button>
          <button id="algorithmResetButton" class="button control-button">
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.5 2 1.5 10 8.5 10 8.5 2 7.5 2 7.5 9 2.5 9 2.5 2ZM0.5 1 9.5 1 9.5 2 0.5 2ZM4 1 6 1 6 0 6 0 4 0ZM3 3 3 8 4 8 4 3ZM6 3 6 8 7 8 7 3ZM4.5 3 4.5 8 5.5 8 5.5 3Z" fill="#1f1f1f" />
            </svg>
          </button>
        </div>
      </div>
      <figcaption style="position:absolute; width: 380px; top:485px; left:594px; text-align: left;">

        Machine learning is susceptible to poisoning attacks in which
        adversaries inject maliciously crafted training data into the
        training set to induce specific model behavior.

        We focus on <em>subpopulation attacks</em>, in which the attacker's goal is to induce a model that produces a targeted and incorrect output (label <font color="steelblue">blue</font> in our demos) for a particular subset of the input space (colored <font color="orange">orange</font>). We study the question, <em>which subpopulations are the most vulnerable to an attack and why</em>?
        <figcaption>
    </figure>
  </d-figure>

  <d-article>
    <p>
      Machine learning is susceptible to poisoning attacks, in which an attacker controls a small fraction of the training data and chooses that data with the goal of inducing some behavior (unintended by the model developer) in the trained model <d-cite key="10.5555/1387709.1387716, biggio2012poisoning"></d-cite>. Previous works have mostly considered two extreme attacker objectives: <em>indiscriminate</em> attacks, where the attacker's goal is to reduce overall model accuracy <d-cite key="biggio2012poisoning, 10.5555/3007337.3007488, 10.5555/2886521.2886721, steinhardt2017certified, koh2018stronger"></d-cite>, and <em>instance-targeted</em> attacks, where the attacker's goal is to reduce accuracy on a specific known instance <d-cite key="shafahi2018poison, zhu2019transferable, koh2017understanding, geiping2020witches, huang2020metapoison"></d-cite>. Recently, Jagielski et al. introduced the <em>subpopulation</em> attack, a more realistic setting in which the adversary attempts to control the model’s behavior on a specific subpopulation <d-cite key="jagielski2021subpopulation"></d-cite> while having negligible impact on the model’s performance on the rest of the population. Such attacks are more realistic &mdash; for example, the subpopulation may be a type of malware produced by the adversary that they wish to have classified as benign, or a type of demographic individual for which they want to increase (or decrease) the likelihood of being selected by an employment screening model &mdash; and are harder to detect than indiscriminate attacks.
    </p>
    <p>

      In this article, we present visualizations to understand poisoning attacks in a simple two-dimensional setting, and to explore a question about poisoning attacks against subpopulations of a data distribution: <em>how do subpopulation characteristics affect attack difficulty</em>? We visually explore these attacks by animating a poisoning attack algorithm in a simplified setting, and quantifying the difficulty of the attacks in terms of the properties of the subpopulations they are against.
    </p>

    <h1>Problem Setup</h1>

    <p>
      We consider a binary prediction task $h : \mathcal{X} \rightarrow \mathcal{Y},$ where $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} = \{+1, -1\},$ and where the prediction model $h$ is characterized by the model parameters $\theta \in \Theta \subseteq \mathbb{R}^d.$ We denote the non-negative convex loss on a point $(x, y) \in \mathcal{X} \times \mathcal{Y}$ by $l(\theta; x, y)$, and define the loss over a set of points $A$ as $L(\theta; A) = \sum_{(x, y) \in A} l(\theta; x, y)$.
    </p>

    <p>
      We use the <em>model-targeted</em> poisoning attack design from Suya et al. <d-cite key="suya2021modeltargeted"></d-cite>, which is shown to have state-of-the-art performance on subpopulation attacks. In a model-targeted poisoning attack, the attacker's objective is captured in a target model. The goal is to induce a model as close as possible to that target model. By describing the objective using a target model, complex objectives can be captured without needing to design customized attacks. [TODO (Suya) - need a bit more motivation here, but I think overall should try to condense this section - we want to get to the dataset parameter space figure more quickly. Maybe, can separate the description of the datasets to get to that figure first, before talking about the poisoning attack algorithm, and then introduce the poisoning attacks, and then to integrate the description of the attacks with the first visualization.]

      We also describe the poisoning attack process with the following game-theoretic formalization:
    </p>

    <ol>
      <li>
        $N$ data points are drawn from the true data distribution over $\mathcal{X} \times \mathcal{Y}$ to produce the clean training set $\mathcal{D}_c.$
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c$ and the model space $\Theta$, generates a target classifier $\theta_p \in \Theta$ which satisfies the attack objective.
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c,$ $\Theta,$ $\theta_p,$ and the training process, produces a set of poisoning points $\mathcal{D}_p.$
      </li>
      <li>
        The model builder trains on $\mathcal{D}_c \cup \mathcal{D}_p$ to produce the induced classifier $\theta_{atk}.$
      </li>
    </ol>

    <p>
      We next describe the components we use in our experiments for each part of the above framework, as well as the formation of the subpopulations.
    </p>

    <h3>Datasets</h3>

    <p>
      We study poisoning attacks against two different types of datasets. First, we generate synthetic datasets using dataset generation algorithms from Scikit-learn <d-cite key="scikit-learn"></d-cite>. Each of these datasets is controlled by a set of parameters, which captures different global dataset properties (e.g., separation between different classes). The synthetic datasets are also limited to just two features, so that direct visualizations of the attacks are possible. Second, we use the UCI Adult dataset <d-cite key="Dua:2019"></d-cite>, which has been used previously in the evaluations of subpopulation poisoning attacks <d-cite key="suya2021modeltargeted, jagielski2021subpopulation"></d-cite>. The Adult dataset is of much higher dimension (57 after data transformations), and so the attack process cannot be visualized directly as in case of synthetic datasets. The purpose of this dataset is to gauge the attack behavior in a more complicated and practical setting.
    </p>

    <h3>Subpopulation Formation and Adversary Goal</h3>

    <p>
      We consider two subpopulation generation processes: clustering and semantic generation. Cluster subpopulations are chosen by using a k-means clustering algorithm to divide the training set into $k$ different clusters (ClusterMatch in Jagielski et al. <d-cite key="jagielski2021subpopulation"></d-cite>). Then we only consider instances with negative labels (in binary classification tasks) from each cluster and form the final cluster subpopulations. The attacker objective is to have the induced model misclassify the entire subpopulation into the positive class. This generation process is the same as the one used in Suya et al. <d-cite key="suya2021modeltargeted"></d-cite>, and is useful since it allows us to measure the success of the attack without any ambiguity using the accuracy of the induced model on the subpopulation. A completely successful attack attains 0% test accuracy on the target subpopulation, and the attack success may, in general, be measured as the error rate on the target subpopulation.
    </p>

    <p>
      In the semantic generation setting, the adversary cares about some semantic property possessed by certain instances in the dataset. Semantic subpopulations are generated by using a feature-matching algorithm (FeatureMatch in Jagielski et al.<d-cite key="jagielski2021subpopulation"></d-cite>) to obtain those instances which satisfy the semantic property and again taking only the instances with a negative label. This subpopulation generation process reflects a more realistic attacker objective, since an attacker is likely to care about influencing model behavior on a subpopulation matching a set of meaningful properties. However, this process relies on the existence of semantically meaningful features in the dataset, and so cannot be performed on our synthetic datasets.
    </p>

    <h3>Training Algorithm</h3>

    <p>
      We assume the model builder trains models using empirical risk minimization (ERM) with the following optimization strategy:
    </p>

    <d-math style="margin: auto" block>
      \theta_c = \underset{\theta \in \Theta}{\text{argmin}} \frac{1}{|\mathcal{D}_c|} L(\theta; \mathcal{D}_c) + C_r \cdot R(\theta)
    </d-math>

    <p>
      where $R(\cdot)$ is the non-negative regularization function and $C_r$ is the regularization strength parameter.
    </p>

    <h3>Target Model</h3>

    <p>
      We conduct experiments on linear SVM models. The attack method we choose for visualization (details below) requires inputting a target model as attack parameter. The required target model is generated using the label-flipping attack variant described in Koh et al.<d-cite key="koh2018stronger"></d-cite>, which is also used by Suya et al. in their experiments. As opposed to the more general attack settings where adversaries can control both the features and labels of the poisoning points, label-flipping attackers can only change some fraction of the labels of the clean training set. In the variant described by Koh et al., the adversary randomly chooses some fraction of samples from the target subpopulation of the clean training set, flips their labels, and repeats them some number of times to produce the poisoned training set. The target model for our attack method is chosen to be the induced model from this label-flipping attack. More specifically, for each subpopulation, we use the label-flipping attack to generate a collection of candidate classifiers (using different fraction and repetition number) which each achieve 0% accuracy (100% error rate) on the target subpopulation. Afterwards, the classifier with the lowest loss on the clean training set is chosen to be the target model, as done in Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>.
    </p>

    <h3>Attack Algorithm</h3>

    <p>
      We use the online model-targeted attack algorithm developed by Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>. Given the clean training set, target model, and model training parameters, the attack algorithm produces a set of poisoning points sequentially by maintaining an intermediate induced model and choosing the point that maximizes the loss difference between the intermediate model and the target model. The selected poisoning point is then added to the current training set and the intermediate model is also updated accordingly by the attacker using the knowledge of the model training process. Importantly, the online attack provides theoretical guarantees about convergence of the induced model to the target model as the number of poisoning points increases. Since we have chosen our target model to misclassify the entire subpopulation, this means we have a guarantee that the online attack process will eventually produce a poisoned training set (which may be huge in size) whose induced model can satisfy the attacker objective.
    </p>

    <p>
      A useful consequence of using the online attack algorithm is that the rate of convergence is characterized by the loss difference between the target and clean models on the clean dataset. If we define the <em>loss-based distance</em> $D_{l, \mathcal{X}, \mathcal{Y}} : \Theta \times \Theta \rightarrow \mathbb{R}$ between two models $\theta_1, \theta_2$ over a space $\mathcal{X} \times \mathcal{Y}$ with loss function $l(\theta; x, y)$ by
    </p>

    <d-math style="margin: auto" block>
      D_{l, \mathcal{X}, \mathcal{Y}}(\theta_1, \theta_2) := \max_{(x, y) \in \mathcal{X} \times \mathcal{Y}} l(\theta_1; x, y) - l(\theta_2; x, y),
    </d-math>

    <p>
      then the loss-based distance between the induced model $\theta_{atk}$ and the target model $\theta_p$ correlates to the loss difference $L(\theta_p; \mathcal{D}_c) - L(\theta_c; D_c)$ between $\theta_p$ and the clean model $\theta_c$ on the clean dataset. This fact gives a general heuristic for predicting attack difficulty: the closer a target model is to the clean model as measured by loss difference on the clean dataset (under the same search space of poisoning points), the easier the attack will be. This also justifies the decision to choose a target model with lower loss on the clean dataset.
    </p>

    <p>
      People may wonder why we choose the online model-targeted attack instead of simpler ones such as the label-flipping attacks. In our experiments, the model-targeted attacks were in general much more efficient than the label-flipping attacks to achieve the same attacker objective. Since our eventual goal is to describe attack difficulties by the (essential) number of poisons needed (i.e., number of poisoning points of optimal attacks), more efficient attacks serve as better proxy to the optimal poisoning attacks. Moreover, the model-targeted attack has nice theoretical properties and is a general enough framework to be worthy of study on its own.
    </p>

    <p>
      Now that we have outlined the attack process, we can move on to our first set of experiments on synthetic datasets.
    </p>

    <h1>Synthetic Datasets</h1>

    <h3>Synthetic Dataset Generation</h3>

    <p>
      For the first set of experiments, we consider a family of synthetic datasets resembling Gaussian mixtures with two components. The datasets are controlled by two parameters, which we will refer to as the dataset parameters. The first dataset parameter is the class separation parameter $\alpha \ge 0$ which controls the distance between the two class centers. The second dataset parameter is the label noise parameter $\beta \in [0, 1]$ which controls the fraction of points whose labels are randomly assigned. For fixed dataset parameters, we generate several different datasets by feeding different random seeds. Each of the datasets generated contains $n=3000$ samples and is class-balanced. Each dataset is then divided into training and test sets in a 2:1 ratio.
    </p>

    <h3>Dataset Parameter Space</h3>

    <p>
      We generate the datasets over a grid of the dataset parameters. For each combination of the two parameters, 10 synthetic datasets are created by feeding different random seeds. Before conducting the poisoning attacks on the subpopulations, let’s observe the behavior of the clean models trained on each combination of the dataset parameters.
    </p>

    <d-figure id="svelte-param-space-acc-dfigure" class="param-space-acc">
      <figure>
        <div id="svelte-param-space-acc-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>
          The clean model performance can be affected by the dataset parameters: as the two classes are more separated and less noisy, the clean models achieve higher test accuracy. Click points on the plot to see the datasets generated with each parameter combination.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The overall trends in the plot meet our expectation: clean model accuracy improves as the classes become more separated and exhibit less label noise.
    </p>

    <h3>Attack Visualization</h3>

    <p>
      We can now move onto visualizing the poisoning attacks on these datasets. For each synthetic dataset, we run the k-means clustering algorithm ($k=16$) and extract the negative-label instances from each cluster to form the subpopulations. We then generate target models for each nonempty subpopulation according to the earlier specification (100% error rate on the target subpopulation, measured on the test set). Finally, we use the model-targeted attack mentioned above to generate the poisoning points to induce the model of desired behavior. For our case, the attack terminates when the induced model misclassifies at least 50% of the target subpopulation, measured on the training set.
    </p>

    <p>
      <font color="red">
        [Suya: there is a big jump from previously mentioning that attacker goal is to have 100% attack success, but now we are talking about at least 50% of attack success. It is important to explain why we are doing this].
      </font>
    </p>

    <p>
      <font color="red">
        [Evan: Good point, I'll do that here:]
      </font>
    </p>

    <p>
      Why do we terminate the attack after achieving only 50% attack success? In earlier experiments requiring 100% attack success, we observed that attack difficulty was often determined by outliers in the subpopulation. By relaxing the attack success requirement, we are able to capture the more essential properties of an attack against each subpopulation. Since our eventual goal is to characterize attack difficulty in terms of the properties of the targeted subpopulation (which outliers do not necessarily satisfy), this is a reasonable relaxation.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example1" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>The poisons are added sequentially to induce a model which misclassifies the orange points as positive (i.e., blue color).</figcaption>
    </d-figure>

    <p>
      The above animation shows how the attack sequentially adds poisoning points into the training set to move the induced model towards the target model. Take note of how the poisoning points with positive labels work to reorient the model's decision boundary to cover the subpopulation while minimizing the impact on the rest of the dataset. This behavior also echos with the target model selection criteria mentioned above, which aims to minimize the loss on the clean training set while satisfying the attacker goal. Another possible way to generate the target model is to push the entire clean decision boundary downwards without reorienting it<d-footnote>i.e., the target model differs from the clean model only in the bias term.</d-footnote>, but this would have resulted in a model with higher loss on other parts of the dataset, and thus a higher loss difference to the clean model. Intuitively, such an alternative would experience more "resistance" from the dataset, preventing the induced model from moving as swiftly to the target.
    </p>

    <p>
      <font color="red">
        [Suya: I am not sure what you want to say here, but without actually showing the performance of model-targeted attacks with new target model, it is not informative. Also, target model is something that matters only for the model-targeted attacks]
      </font>
    </p>

    <p>
      <font color="red">
        [Evan: My goal was to highlight the performance of the online attack and the importance of choosing a good target model (as to support our choice of attack framework), but you are right in your points. I'm thinking we should just remove the discission related to the alternative target model?]
      </font>
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example2" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against a dataset with high label noise is easy, and one possible reason is the clean model may not be heavily influenced by the points in the clean training set.</figcaption>
    </d-figure>

    <p>
      Let's look at a low-difficulty attack. The above dataset exhibits a large amount of label noise, and as a result the clean model performs very poorly. When the poisoning points are added, the model is quickly persuaded to change its decision with respect to the target subpopulation. Observe that the poisoned model labels all points in the training set as positive, with the corresponding decision boundary lying out of view. In other words, the attack is changing not just the model behavior local to the target subpopulation, but also the global behavior of the model.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example3" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against a dataset with close class centers is easy, since the linear SVM model does not have sufficient capacity to perform well on the clean training set.</figcaption>
    </d-figure>

    <p>
      The next example shows an attack result similar to the noisy dataset shown above, but now the reason is slightly different: although the clean training set now has small label noise, the model does not have enough capacity to generate a useful classification function. The end result is the same: poisoning points can strongly influence the behavior of the model. Note that in both of the above examples, the two classes are balanced; if the labels of the two classes are represented in a different ratio, the clean model may prefer to classify all points as the dominant label, and the attack difficulty of misclassifying target subpopulation into the minority label may also significantly increase.
    </p>

    <p>
      As mentioned earlier, the properties that apply to the above two datasets should apply to all the datasets with either close class centers or high label noise. We can demonstrate this empirically by looking at the mean attack difficulty over the entire grid of the dataset parameters, where we describe the difficulty of an attack against a dataset of $n$ training samples that uses $p$ poisoning points using the ratio $p/n$. That is, the difficulty of an attack is the fraction of poisoning points added relative to the size of the original clean training set.
    </p>

    <d-figure id="svelte-param-space-dif-dfigure" class="param-space-acc">
      <figure>
        <div id="svelte-param-space-dif-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Average attack difficulty is roughly characterized by clean model accuracy, with more accurate models corresponding to more difficult attacks. Click points on the plot to see different dataset parameter combinations, or click datasets on the right to see specific attacks.</figcaption>
      </figure>
    </d-figure>

    <p>
      It appears that the attacks against datasets with poorly performing clean models tend to be easier than others, and in general attack difficulty increases with clean model accuracy. The behavior is most consistent with clean models of low accuracies, since all attacks are easy and require only a few poisoning points. As the clean model accuracy increases, the behavior becomes more complex and attack difficulty begins to depend more on the properties of the target subpopulation. Since attack difficulty is driven by the loss difference between the target and clean models, it makes sense to try to characterize the behavior of further examples in terms of this quantity.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example4" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack with low collateral damage is easy, since the loss difference between the target and clean models is small.</figcaption>
    </d-figure>

    <p>
      The next attack is easy for the attacker despite being against a dataset with an accurate clean model. The targeted subpopulation lies on the boundary of the class it belongs to, and more specifically the loss difference between the target and clean models is small. In other words, the attack causes very little "collateral damage."
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example5" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
      </figure>
      <figcaption>An attack against the center of a cluster is hard, since the subpopulation is "protected" by surrounding data.</figcaption>
    </d-figure>

    <p>
      This is our first example of an exceptionally difficult attack, requiring over 800 poisons. The loss difference between the target and clean models is high, since the target model misclassifies a large number of points in the negative class. Notice that it seems hard to even produce a target model which does not cause a large amount of collateral damage. In some sense, the subpopulation is well protected and is more robust against poisoning attacks due it its central location.
    </p>

    <h3>Quantitative Analysis</h3>

    <p>
      Visualizations are helpful, but are rarely possible given the high-dimensional nature of most datasets in practice. Now that we have visualized some of the attacks in the lower-dimensional setting, can we quantify the properties that describe the difficulty of poisoning attacks?
    </p>

    <p>
      In the previous section, we made the observation that attack difficulty tends to increase with clean model accuracy. To be a little more precise, we can see how the attack difficulty distribution changes as a function of clean model accuracy, for our attack setup.
    </p>

    <d-figure id="svelte-cleanacc-dif-hist-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-cleanacc-dif-hist-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Datasets with accurate clean models tend to produce attacks with a wider range of difficulties. Datasets with inaccurate clean models tend to mostly produce easy attacks.</figcaption>
      </figure>
    </d-figure>

    <p>
      The histogram empirically verifies some of our observations from earlier: attacks against datasets with inaccurate clean models tend to be easy, while attacks against datasets with accurate clean models can produce a wider range of attack difficulties.
    </p>

    <p>
      Of course, it is desirable to know more than just whether or not it is possible for attack difficulty to vary based on the dataset and clean model. Our real objective is to determine what properties of the subpopulation affect attack difficulty, and how. One way to do this is to gather a numerical description of the targeted subpopulation, and use that data to predict attack difficulty.
    </p>

    <!-- <p>
      Lead into the plot below . . .
    </p> -->

    <font color="red">Evan: I wasn't sure if the below results were worth including, since we never made any strong conclusions about the importance these numeric factors affecting susceptibility. Also, the below figures may cause performance issues since they are SVG elements with over 20K points.</font>

    <d-figure id="svelte-synth-scatter-dfigure" class="dif-scatter">
      <figure>
        <div id="svelte-synth-scatter-target" style="text-align: center;"></div>
        <figcaption>A numerical description of a subpopulation can be used to predict the difficulty of an attack against it. But does it really give a complete picture?</figcaption>
      </figure>
    </d-figure>

    <!-- explain the plot above . . . -->
    <!-- <p>

    </p> -->

    <h1>Adult Dataset</h1>

    <p>
      While the visualizations made possible by the synthetic datasets are useful for developing an intuition for subpopulation poisoning attacks, it is desirable to see how subpopulation susceptibility arises in a more complex and practical setting. For this purpose, we perform subpopulation poisoning attacks against the UCI Adult dataset, whose associated classification task is to determine whether an adult's income exceeds $50,000 per year based on attributes like education, age, race, martial status, etc.
    </p>

    <p>
      Subpopulations for the Adult dataset are selected using semantic generation. To generate a semantic subpopulation, first a subset of categorical features is selected, and specific values are chosen for those features. For example, the categorical features could be chosen to be "work class" and "education level", and the features' values could then be chosen to be "never worked" and "some college", respectively. Then, every negative-label ("$\le$ 50K") instance in the training set matching all of the (feature, label) pairs is extracted to form the subpopulation. The subpopulations for our experiments are chosen by considering every subset of categorical features and every combination of those features which is realized in the training set. For simplicity, we only consider subpopulations with a maximum of three feature selections.
    </p>

    <p>
      In total, 4,338 subpopulations were formed using this method. Each of these subpopulations was attacked using the same strategy as the synthetic subpopulations. Of these attacks, 1,602 were trivial (the clean model satisfied the attack objective), leaving 2,736 nontrivial attacks.
    </p>

    <p>
      Just as with the synthetic datasets, we can examine the attack difficulty distribution for all the nontrivial attacks:
    </p>

    <d-figure id="svelte-adult-dif-hist-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-dif-hist-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Attacks in a more practical setting still yield an interesting distribution of attack difficulties.</figcaption>
      </figure>
    </d-figure>

    <p>
      In the above histogram, we see a wide range of attack difficulties. Note that the overall low difficulties as measured by the ratio $p / n$ still represent a significant contribution from the attacker, since $n = 15,682$ for the Adult dataset (for example, a difficulty of 0.05 corresponds to ~780 poisons).
    </p>

    <p>
      For completeness, we can also plot the attack difficulty against numerical properties of the Adult dataset subpopulations:
    </p>

    <d-figure id="svelte-adult-scatter-dfigure" class="dif-scatter">
      <figure>
        <div id="svelte-adult-scatter-target" style="text-align: center;"></div>
        <figcaption>Caption</figcaption>
      </figure>
    </d-figure>

    <h3>Semantic Subpopulation Characteristics</h3>

    <p>
      Recall that our goal is to determine how subpopulation characteristics affect attack difficulty. While this question can be posed in terms of the statistical properties of the subpopulation (e.g., by examining its size or label ratio), it is also interesting to ask which semantic properties of the subpopulation contribute to attack difficulty. In this section, we explore the relationships between attack difficulty and the semantic properties of the targted subpopulation.
    </p>

    <p>
      To start, we can compare a few attacks from our experiments on the Adult datastet.
    </p>

    <d-figure id="svelte-adult-comparison1-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-comparison1-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>How do semantic properties of a subpopulation affect attack difficulty? All of the above subpopulations are of similar sizes and are classified with 100% accuracy by the clean model.</figcaption>
      </figure>
    </d-figure>

    <p>
      The above subpopulations are all classified with 100% accuracy by the clean model and are of similar sizes (between 1% and 2% of the training set). So what makes some of the attacks more or less difficult? In the attacks shown above, the differences may be related to the points surrounding the subpopulation. For our experiments, we defined the subpopulations by taking only negative-label instances satisfying some semantic property. If we remove the label restriction, we gain a more complete view of the surrounding points, and, in particular, can consider statistics (like label ratio) of the ambient points.
    </p>

    <p>
      For a semantic subpopulation with semantic property $P$, let us call the set of all points satisfying $P$ the <em>ambient subpopulation</em> (since it also includes positive-label points), and call the fraction of points in the ambient subpopulation with a positive label the <em>ambient positivity</em> of the subpopulation. In the above attacks, attack difficulty is closely correlated with the ambient positivity of the subpopulation. This makes sense, since positive-label points near the subpopulation work to the advantage of the attacker when attempting to induce misclassification of the negative-label points. Stated in terms of the model-targeted attack, if the clean model classifies the ambient subpopulation as the negative-label, then the loss difference between the target and clean models is smaller if there are positive-label points in that region.
    </p>

    <font color="red">Here, I'll plot the ambient positivity (we just called it "positivity") as well as model loss difference for the attacks above.</font>

    <p>
      But does the ambient positivity of a subpopulation necessarily determine attack difficulty for otherwise similar subpopulations? If we restrict our view to subpopulations with similar pre-poisoning ambient positivity (say, between 0.2 and 0.3), we still find a significant spread of attack difficulties:
    </p>

    <d-figure id="svelte-adult-comparison2-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-comparison2-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Even with similar size, ambient positivity, and clean model performance, subpopulations can experience differences in attack difficulty.</figcaption>
      </figure>
    </d-figure>

    <p>
      Are these differences in attack difficulties just outliers due to our particular experiment setup, or is there some essential difference between the subpopulations which is not captured by their numerical descriptions? Furthermore, if such differences do exist, can they be described using the natural semantic meaning of the subpopulations?
    </p>

    <!-- by feature -->
    <!-- It'd be nice to be able to rank each feature and label pair in order of 
    increasing susceptibility, but I don't think we had sufficient data to make
    any interesting claims -->


    <!-- <p>
      We should also be able to determine which groups (as described by categorical features of a dataset) are generally more susceptible to a poisoning attack, even when the precise subpopulation can vary. One way to gauge this behavior 
    </p> -->

    <p>
      What happens when two semantic subpopulations match on the same features, but differ in only a single feature's value? For example, consider the two attacks below:
    </p>

    <d-figure id="svelte-adult-comparison3-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-comparison3-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>What can we learn about a feature's effect on attack difficulty by varying that feature's value in the subpopulation? The above subpopulations require significantly different attacks as measured by difficulty, yet differ only slightly in semantic description.</figcaption>
      </figure>
    </d-figure>

    <p>
      In the above example, the subpopulations possess very similar semantic descriptions, only differing in the value for the "relationship status" feature. Furthermore, the subpopulations are similarly sized with respect to the dataset (1.3% and 1.1% of the training set, respectively), and each subpopulation is perfectly classified by the clean model. Yet the first subpopulation required significantly more poisoning points, at least in the model-targeted attack.
    </p>

    <h1>Discussion</h1>

    <p>
      A natural objection to this analysis is that the linear SVM is not a sufficiently complex model to produce meaningful results about subpopulation susceptibility. However, our observations in this paper are still very meaningful: first, simple and low-capacity models are still widely used in practice due to their ease of use, low computational cost and effectiveness<d-cite key="decrema2019progress, tramer2021differentially"></d-cite>, and so our simplified analysis is still relevant in practice. Second, kernel methods are powerful tools to handle non-linearly separable datasets by projecting them into a linearly separable high-dimensional space and are widely adopted in practice. Therefore, if the important spatial relationships among the data points are still preserved after projection, then the same conclusions reached in our simplified setting still apply to the more complex setting by examining the spatial relationships in the transformed space.
    </p>

    <p>
      Another objection is that the model-targeted attack may not adequately describe the difficulty of any attack against a subpopulation, since a more optimal attack may exist that uses a different target model or some completely different attack framework.
    </p>

    <p>
      One open question is how subpopulation susceptibility depends on label ratio of the training data. In our experiments, all the examined datasets are class-balanced. What happens when this is not the case? Do subpopulations from parts of the input space lacking representation in the training data admit easier poisoning attacks?
    </p>

    <!-- <h1>Conclusion</h1> -->
    <!-- I think conclusion should go with discussion? -->

  </d-article>

  <d-appendix>
    <!-- <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to...
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>